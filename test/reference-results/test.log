[92mannotate-ref-simu[0m                ./bin/partis annotate --plot-annotation-performance --plotdir test/new-results/annotate-ref-simu-annotation-performance --only-csv-plots --only-overall-plots --is-simu --sw-cachefname test/reference-results/test/parameters/simu/sw-cache.yaml --infname test/reference-results/test/simu.yaml --parameter-dir test/reference-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/annotate-ref-simu.yaml
annotating
smith-waterman
  vsearch: 2399 / 2557 v annotations (158 failed) with 47 v genes in 0.5 sec
        reading sw results from test/reference-results/test/parameters/simu/sw-cache.yaml
      info for 2557 / 2557 = 1.000   (0 failed, 0 duplicates)
      kept 1486 (0.581) unproductive
  plotting performance 
    [93mwarning[0m skipped annotation performance evaluation on 15 queries with different true and inferred net shm indel lengths: 8909806169164714756 3411555643997713375 -1223213546020366966 -5730441331628128067 2447496209938796791 6480740913865585177 2712916551846080017 -2050692363108552769 -2794925622856437047 3501569511712948868 -5612009640891275188 -8882582160216389456 -3480309570711181613 -5810839868584951332 -9191880305116534813
(0.3 sec)
        water time: 2.8
hmm
    prepare_for_hmm: (0.3 sec)
    running 10 procs
                    calcd:         vtb 2557        fwd    0
             min-max time:  6.9 - 7.5 sec
    read output
  plotting performance (0.3 sec)
        processed 2557 hmm output lines with 2557 sequences in 2557 events  (0 failures)
         infra time: 4.8
      hmm step time: 12.4
      total time: 17.3
[92mmulti-annotate-ref-simu[0m          ./bin/partis annotate --plot-annotation-performance --simultaneous-true-clonal-seqs --plotdir test/new-results/multi-annotate-ref-simu-annotation-performance --only-csv-plots --only-overall-plots --is-simu --sw-cachefname test/reference-results/test/parameters/simu/sw-cache.yaml --infname test/reference-results/test/simu.yaml --parameter-dir test/reference-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/multi-annotate-ref-simu.yaml
annotating
smith-waterman
  vsearch: 2399 / 2557 v annotations (158 failed) with 47 v genes in 0.4 sec
        reading sw results from test/reference-results/test/parameters/simu/sw-cache.yaml
      info for 2557 / 2557 = 1.000   (0 failed, 0 duplicates)
      kept 1486 (0.581) unproductive
  plotting performance 
    [93mwarning[0m skipped annotation performance evaluation on 15 queries with different true and inferred net shm indel lengths: 8909806169164714756 3411555643997713375 -1223213546020366966 -5730441331628128067 2447496209938796791 6480740913865585177 2712916551846080017 -2050692363108552769 -2794925622856437047 3501569511712948868 -5612009640891275188 -8882582160216389456 -3480309570711181613 -5810839868584951332 -9191880305116534813
(0.3 sec)
        water time: 2.7
hmm
  [93mwarning[0m split apart 6 clusters that contained multiple cdr3 lengths (total clusters: 482 --> 489)
    prepare_for_hmm: (0.2 sec)
    running 10 procs
                    calcd:         vtb  489        fwd    0
             min-max time:  3.9 - 6.5 sec
    read output
  plotting performance (0.4 sec)
        processed 489 hmm output lines with 2557 sequences in 489 events  (0 failures)
         infra time: 3.4
      hmm step time: 10.0
      total time: 14.4
[92mpartition-ref-simu[0m               ./bin/partis partition --n-max-queries 500 --n-precache-procs 10 --plot-annotation-performance --biggest-logprob-cluster-to-calculate 5 --biggest-naive-seq-cluster-to-calculate 5 --persistent-cachefname test/new-results/cache-ref-partition.csv --plotdir test/new-results/partition-ref-simu-annotation-performance --only-csv-plots --only-overall-plots --is-simu --sw-cachefname test/reference-results/test/parameters/simu/sw-cache.yaml --infname test/reference-results/test/simu.yaml --parameter-dir test/reference-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/partition-ref-simu.yaml
  --n-max-queries: stopped after reading 500 queries from input file
partitioning
smith-waterman
  vsearch: 475 / 500 v annotations (25 failed) with 30 v genes in 1.5 sec
        reading sw results from test/reference-results/test/parameters/simu/sw-cache.yaml
      info for 500 / 500 = 1.000   (0 failed, 0 duplicates)
      kept 260 (0.520) unproductive
  plotting performance 
    [93mwarning[0m skipped annotation performance evaluation on 9 queries with different true and inferred net shm indel lengths: 8909806169164714756 3411555643997713375 -1223213546020366966 -5730441331628128067 2447496209938796791 6480740913865585177 2712916551846080017 -2050692363108552769 -2794925622856437047
(0.3 sec)
        water time: 1.5
hmm
caching all 500 naive sequences
    prepare_for_hmm: (0.1 sec)
    running 10 procs
                no/empty cache file
                    calcd:         vtb 500        fwd   0
                   merged:       hfrac   0     lratio   0
             min-max time:  3.0 - 3.8 sec
         infra time: 0.2
      hmm step time: 5.1
   collapsed 500 queries into 290 clusters with identical naive seqs (0.0 sec)
290 clusters with 10 procs
    prepare_for_hmm: (0.1 sec)
       naive hfrac bounds: 0.015 0.106   (0.105 mean mutation in parameter dir test/reference-results/test/parameters/simu/hmm)
    running 10 procs
          read from cache:  naive-seqs 500   logprobs   0
                    calcd:         vtb 118        fwd 160
                   merged:       hfrac  27     lratio  27
             min-max time:  1.5 - 4.0 sec
         infra time: 0.3
      hmm step time: 4.8
236 clusters with 7 procs
    prepare_for_hmm: (0.1 sec)
    running 7 procs
          read from cache:  naive-seqs 629   logprobs 160
                    calcd:         vtb  34        fwd  83
                   merged:       hfrac  12     lratio  16
             min-max time:  0.8 - 4.7 sec
         infra time: 0.2
      hmm step time: 5.4
208 clusters with 5 procs
    prepare_for_hmm: (0.1 sec)
    running 5 procs
          read from cache:  naive-seqs 666   logprobs 243
                    calcd:         vtb  34        fwd  64
                   merged:       hfrac  22     lratio  11
             min-max time:  1.5 - 4.6 sec
         infra time: 0.2
      hmm step time: 5.1
175 clusters with 3 procs
    prepare_for_hmm: (0.1 sec)
    running 3 procs
          read from cache:  naive-seqs 706   logprobs 307
                    calcd:         vtb  17        fwd  53
                   merged:       hfrac  16     lratio   4
             min-max time:  3.3 - 4.6 sec
         infra time: 0.2
      hmm step time: 5.0
155 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 730   logprobs 360
                    calcd:         vtb  27        fwd  46
                   merged:       hfrac  21     lratio   9
             min-max time:  3.9 - 7.8 sec
         infra time: 0.2
      hmm step time: 8.4
125 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 769   logprobs 406
                    calcd:         vtb   8        fwd  29
                   merged:       hfrac   4     lratio   4
             min-max time:  3.0 - 3.4 sec
         infra time: 0.1
      hmm step time: 3.8
117 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 779   logprobs 435
                    calcd:         vtb   5        fwd  14
                   merged:       hfrac   4     lratio   2
             min-max time:  1.7 - 2.4 sec
         infra time: 0.1
      hmm step time: 2.7
111 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 786   logprobs 449
                    calcd:         vtb   1        fwd  13
                   merged:       hfrac   0     lratio   1
             min-max time:  1.5 - 2.3 sec
         infra time: 0.2
      hmm step time: 2.7
110 clusters with 1 proc
    prepare_for_hmm: (0.1 sec)
    running 1 proc
          read from cache:  naive-seqs 787   logprobs 462
                    calcd:         vtb   2        fwd  72
                   merged:       hfrac   3     lratio   0
                     time:  9.7 sec
      hmm step time: 9.9
  [93mnote[0m not merging entire cpath history
      loop time: 47.8
getting annotations for final partition
    prepare_for_hmm: (0.1 sec)
    running 10 procs
                    calcd:         vtb 107        fwd   0
             min-max time:  1.3 - 2.4 sec
      hmm step time: 3.0
    read output
  plotting performance (0.4 sec)
        processed 107 hmm output lines with 500 sequences in 107 events  (0 failures)
  --only-csv-plots not implemented for partition plots, so skipping
      total time: 60.5
[92mseed-partition-ref-simu[0m          ./bin/partis partition --n-max-queries 500 --persistent-cachefname test/new-results/cache-ref-partition.csv --is-simu --sw-cachefname test/reference-results/test/parameters/simu/sw-cache.yaml --infname test/reference-results/test/simu.yaml --parameter-dir test/reference-results/test/parameters/simu --seed-unique-id -237679582940728480 --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/seed-partition-ref-simu.yaml
  --n-max-queries: stopped after reading 500 queries from input file
partitioning
smith-waterman
  vsearch: 475 / 500 v annotations (25 failed) with 30 v genes in 0.8 sec
        reading sw results from test/reference-results/test/parameters/simu/sw-cache.yaml
      info for 500 / 500 = 1.000   (0 failed, 0 duplicates)
      kept 260 (0.520) unproductive
      removed 431 / 500 = 0.86 sequences with cdr3 length different from seed sequence (leaving 69)
        water time: 0.3
hmm
   collapsed 69 queries into 41 clusters with identical naive seqs (0.0 sec)
41 clusters with 10 procs
    prepare_for_hmm: (0.0 sec)
       naive hfrac bounds: 0.015 0.106   (0.105 mean mutation in parameter dir test/reference-results/test/parameters/simu/hmm)
    running 10 procs
          read from cache:  naive-seqs 792   logprobs 534
                    calcd:         vtb  13        fwd   7
                   merged:       hfrac   4     lratio   0
             min-max time:  0.0 - 0.9 sec
         infra time: 0.1
      hmm step time: 1.3
46 clusters with 7 procs
    prepare_for_hmm: (0.0 sec)
    running 7 procs
          read from cache:  naive-seqs 805   logprobs 541
                    calcd:         vtb   0        fwd   1
                   merged:       hfrac   0     lratio   0
             min-max time:  0.0 - 0.6 sec
         infra time: 0.1
      hmm step time: 0.8
43 clusters with 5 procs
    prepare_for_hmm: (0.0 sec)
    running 5 procs
          read from cache:  naive-seqs 805   logprobs 542
                    calcd:         vtb   0        fwd   3
                   merged:       hfrac   0     lratio   0
             min-max time:  0.0 - 0.6 sec
      hmm step time: 0.7
      removed 62 sequences in unseeded clusters, split 5 seeded clusters into 7 singletons, and merged these into 5 clusters with identical naive seqs
        new n_procs 1 (initial seqs/proc: 6.90   new seqs/proc: 5.00
5 clusters with 1 proc
    prepare_for_hmm: (0.0 sec)
    running 1 proc
          read from cache:  naive-seqs 805   logprobs 545
                    calcd:         vtb   3        fwd   6
                   merged:       hfrac   4     lratio   0
                     time:  0.3 sec
      hmm step time: 0.4
      loop time: 3.2
getting annotations for final partition
    prepare_for_hmm: (0.0 sec)
    running 1 proc
                    calcd:         vtb   1        fwd   0
                     time:  0.2 sec
      hmm step time: 0.2
    read output
        processed 1 hmm output lines with 7 sequences in 1 events  (0 failures)
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
      total time: 5.0
[92mvsearch-partition-ref-simu[0m       ./bin/partis partition --naive-vsearch --n-max-queries 500 --persistent-cachefname test/new-results/cache-ref-partition.csv --is-simu --sw-cachefname test/reference-results/test/parameters/simu/sw-cache.yaml --infname test/reference-results/test/simu.yaml --parameter-dir test/reference-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/vsearch-partition-ref-simu.yaml
  --n-max-queries: stopped after reading 500 queries from input file
partitioning
smith-waterman
  vsearch: 475 / 500 v annotations (25 failed) with 30 v genes in 1.2 sec
        reading sw results from test/reference-results/test/parameters/simu/sw-cache.yaml
      info for 500 / 500 = 1.000   (0 failed, 0 duplicates)
      kept 260 (0.520) unproductive
        water time: 0.3
hmm
       naive hfrac bounds: 0.031 0.031   (0.105 mean mutation in parameter dir test/reference-results/test/parameters/simu/hmm)
        collapsed 500 sequences into 290 unique naive sequences
    using hfrac bound for vsearch 0.031
    running vsearch 20 times (once for each cdr3 length class): . . . . . . . . . . . . . . . . . . . . 
      vsearch time: 4.5
getting annotations for final partition
    prepare_for_hmm: (0.1 sec)
    running 10 procs
                    calcd:         vtb 124        fwd   0
             min-max time:  1.8 - 2.8 sec
      hmm step time: 3.2
    read output
        processed 124 hmm output lines with 500 sequences in 124 events  (0 failures)
      total time: 9.8
[92mcache-parameters-data[0m            ./bin/partis cache-parameters --plotdir test/new-results/test/plots/data --only-csv-plots --only-overall-plots --sw-cachefname test/new-results/test/parameters/data/sw-cache.yaml --infname test/mishmash.fa --parameter-dir test/new-results/test/parameters/data --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human
  note: running on a lot of sequences (1412) without setting --outfname. Which is ok, but there will be no persistent record of the results (except the parameter directory).
caching parameters
  vsearch: 1268 / 1412 v annotations (144 failed) with 177 v genes in 0.9 sec
    keeping 61 / 204 v genes
smith-waterman  (new-allele fitting)
  vsearch: 1264 / 1412 v annotations (148 failed) with 61 v genes in 0.8 sec
  removing old sw cache test/new-results/test/parameters/data/sw-cache.yaml
    running 10 procs for 1412 seqs
    running 13 procs for 71 seqs
      info for 1370 / 1412 = 0.970   (42 failed)
      kept 437 (0.309) unproductive
      removed 23 / 1370 = 0.02 duplicate sequences after trimming framework insertions (leaving 1347)
    water time: 3.0  (ig-sw 0.1  processing 0.2)
  no queries for allele finding
    <only_csv> not yet implemented in allelefinder
smith-waterman  (writing parameters)
  vsearch: 1264 / 1412 v annotations (148 failed) with 61 v genes in 0.2 sec
    running 10 procs for 1370 seqs
    running 13 procs for 29 seqs
      info for 1370 / 1412 = 0.970   (42 failed)
      kept 437 (0.309) unproductive
      removed 23 / 1370 = 0.02 duplicate sequences after trimming framework insertions (leaving 1347)
        writing sw results to test/new-results/test/parameters/data/sw-cache.yaml
  plotting parameters in test/new-results/test/plots/data/sw (3.2 sec)
    writing parameters to test/new-results/test/parameters/data/sw (2.2 sec)
    water time: 9.3  (ig-sw 6.5  processing 0.2)
  writing hmms (0.7 sec)
hmm
    prepare_for_hmm: (0.1 sec)
    running 10 procs
                    calcd:         vtb 1347        fwd    0
             min-max time:  6.6 - 15.1 sec
    read output
  plotting parameters in test/new-results/test/plots/data/hmm (3.5 sec)
    writing parameters to test/new-results/test/parameters/data/hmm (2.1 sec)
        processed 1347 hmm output lines with 1344 sequences in 1344 events  (3 failures)
            [91mwarning[0m skipped 3 invalid events
         infra time: 8.1
      hmm step time: 23.3
  writing hmms (0.7 sec)
      total time: 40.0
[92msimulate[0m                         ./bin/partis simulate --n-sim-events 500 --n-trees 500 --n-leaves 5 --parameter-dir test/new-results/test/parameters/data --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/test/simu.yaml --indel-frequency 0.01 --indel-location v
simulating
   read 500 events from 10 csv files
      total time: 7.2
[92mcache-parameters-simu[0m            ./bin/partis cache-parameters --plotdir test/new-results/test/plots/simu --only-csv-plots --only-overall-plots --is-simu --sw-cachefname test/new-results/test/parameters/simu/sw-cache.yaml --infname test/new-results/test/simu.yaml --parameter-dir test/new-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human
  note: running on a lot of sequences (2557) without setting --outfname. Which is ok, but there will be no persistent record of the results (except the parameter directory).
caching parameters
  vsearch: 2401 / 2557 v annotations (156 failed) with 102 v genes in 0.3 sec
    keeping 47 / 204 v genes
    [91mmissing[0m 9 simulation genes (counts): [95mh[0m[91mv[0m[95m2[0m[95m-26[0m[93m01[0m 1  [95mh[0m[91mv[0m[95m2[0m[95m-5[0m[93m08[0m 0  [95mh[0m[91mv[0m[95m3[0m[95m-11[0m[93m06[0m 0  [95mh[0m[91mv[0m[95m3[0m[95m-13[0m[93m01[0m 0  [95mh[0m[91mv[0m[95m3[0m[95m-13[0m[93m02[0m 0  [95mh[0m[91mv[0m[95m3[0m[95m-43[0m[93m02[0m 1  [95mh[0m[91mv[0m[95m4[0m[95m-30-4[0m[93m01[0m 0  [95mh[0m[91mv[0m[95m4[0m[95m-30-4[0m[93m07[0m 6  [95mh[0m[91mv[0m[95m4[0m[95m-31[0m[93m02[0m 0
smith-waterman  (new-allele fitting)
  vsearch: 2399 / 2557 v annotations (158 failed) with 47 v genes in 0.6 sec
  removing old sw cache test/new-results/test/parameters/simu/sw-cache.yaml
    running 10 procs for 2557 seqs
    running 12 procs for 18 seqs
      info for 2557 / 2557 = 1.000   (0 failed)
      kept 1486 (0.581) unproductive
    water time: 6.2  (ig-sw 0.1  processing 0.1)
    <only_csv> not yet implemented in allelefinder
smith-waterman  (writing parameters)
  vsearch: 2399 / 2557 v annotations (158 failed) with 47 v genes in 0.6 sec
    running 10 procs for 2557 seqs
    running 12 procs for 18 seqs
      info for 2557 / 2557 = 1.000   (0 failed)
      kept 1486 (0.581) unproductive
        writing sw results to test/new-results/test/parameters/simu/sw-cache.yaml
  plotting parameters in test/new-results/test/plots/simu/sw (3.6 sec)
    writing parameters to test/new-results/test/parameters/simu/sw (2.0 sec)
    water time: 14.5  (ig-sw 8.2  processing 0.3)
  writing hmms (2.1 sec)
hmm
    prepare_for_hmm: (0.3 sec)
    running 10 procs
                    calcd:         vtb 2557        fwd    0
             min-max time:  7.5 - 8.3 sec
    read output
  plotting parameters in test/new-results/test/plots/simu/hmm (3.3 sec)
  plotting parameters in test/new-results/test/plots/simu/true (3.7 sec)
    writing parameters to test/new-results/test/parameters/simu/hmm (2.0 sec)
    writing parameters to test/new-results/test/parameters/simu/true (1.4 sec)
        processed 2557 hmm output lines with 2557 sequences in 2557 events  (0 failures)
         infra time: 19.0
      hmm step time: 27.6
  writing hmms (1.4 sec)
      total time: 56.7
[92mannotate-new-simu[0m                ./bin/partis annotate --plot-annotation-performance --plotdir test/new-results/annotate-new-simu-annotation-performance --only-csv-plots --only-overall-plots --is-simu --sw-cachefname test/new-results/test/parameters/simu/sw-cache.yaml --infname test/new-results/test/simu.yaml --parameter-dir test/new-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/annotate-new-simu.yaml
annotating
smith-waterman
  vsearch: 2399 / 2557 v annotations (158 failed) with 47 v genes in 0.6 sec
        reading sw results from test/new-results/test/parameters/simu/sw-cache.yaml
      info for 2557 / 2557 = 1.000   (0 failed, 0 duplicates)
      kept 1486 (0.581) unproductive
  plotting performance 
    [93mwarning[0m skipped annotation performance evaluation on 15 queries with different true and inferred net shm indel lengths: 8909806169164714756 3411555643997713375 -1223213546020366966 -5730441331628128067 2447496209938796791 6480740913865585177 2712916551846080017 -2050692363108552769 -2794925622856437047 3501569511712948868 -5612009640891275188 -8882582160216389456 -3480309570711181613 -5810839868584951332 -9191880305116534813
(0.3 sec)
        water time: 2.9
hmm
    prepare_for_hmm: (0.3 sec)
    running 10 procs
                    calcd:         vtb 2557        fwd    0
             min-max time:  6.8 - 7.4 sec
    read output
  plotting performance (0.3 sec)
        processed 2557 hmm output lines with 2557 sequences in 2557 events  (0 failures)
         infra time: 4.8
      hmm step time: 12.4
      total time: 17.6
[92mmulti-annotate-new-simu[0m          ./bin/partis annotate --plot-annotation-performance --simultaneous-true-clonal-seqs --plotdir test/new-results/multi-annotate-new-simu-annotation-performance --only-csv-plots --only-overall-plots --is-simu --sw-cachefname test/new-results/test/parameters/simu/sw-cache.yaml --infname test/new-results/test/simu.yaml --parameter-dir test/new-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/multi-annotate-new-simu.yaml
annotating
smith-waterman
  vsearch: 2399 / 2557 v annotations (158 failed) with 47 v genes in 0.5 sec
        reading sw results from test/new-results/test/parameters/simu/sw-cache.yaml
      info for 2557 / 2557 = 1.000   (0 failed, 0 duplicates)
      kept 1486 (0.581) unproductive
  plotting performance 
    [93mwarning[0m skipped annotation performance evaluation on 15 queries with different true and inferred net shm indel lengths: 8909806169164714756 3411555643997713375 -1223213546020366966 -5730441331628128067 2447496209938796791 6480740913865585177 2712916551846080017 -2050692363108552769 -2794925622856437047 3501569511712948868 -5612009640891275188 -8882582160216389456 -3480309570711181613 -5810839868584951332 -9191880305116534813
(0.4 sec)
        water time: 2.6
hmm
  [93mwarning[0m split apart 6 clusters that contained multiple cdr3 lengths (total clusters: 482 --> 489)
    prepare_for_hmm: (0.2 sec)
    running 10 procs
                    calcd:         vtb  489        fwd    0
             min-max time:  4.1 - 6.8 sec
    read output
  plotting performance (0.3 sec)
        processed 489 hmm output lines with 2557 sequences in 489 events  (0 failures)
         infra time: 3.3
      hmm step time: 10.3
      total time: 14.6
[92mpartition-new-simu[0m               ./bin/partis partition --n-max-queries 500 --n-precache-procs 10 --plot-annotation-performance --biggest-logprob-cluster-to-calculate 5 --biggest-naive-seq-cluster-to-calculate 5 --persistent-cachefname test/new-results/cache-new-partition.csv --plotdir test/new-results/partition-new-simu-annotation-performance --only-csv-plots --only-overall-plots --is-simu --sw-cachefname test/new-results/test/parameters/simu/sw-cache.yaml --infname test/new-results/test/simu.yaml --parameter-dir test/new-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/partition-new-simu.yaml
  --n-max-queries: stopped after reading 500 queries from input file
partitioning
smith-waterman
  vsearch: 475 / 500 v annotations (25 failed) with 30 v genes in 0.2 sec
        reading sw results from test/new-results/test/parameters/simu/sw-cache.yaml
      info for 500 / 500 = 1.000   (0 failed, 0 duplicates)
      kept 260 (0.520) unproductive
  plotting performance 
    [93mwarning[0m skipped annotation performance evaluation on 9 queries with different true and inferred net shm indel lengths: 8909806169164714756 3411555643997713375 -1223213546020366966 -5730441331628128067 2447496209938796791 6480740913865585177 2712916551846080017 -2050692363108552769 -2794925622856437047
(0.4 sec)
        water time: 1.4
hmm
caching all 500 naive sequences
    prepare_for_hmm: (0.1 sec)
    running 10 procs
                no/empty cache file
                    calcd:         vtb 500        fwd   0
                   merged:       hfrac   0     lratio   0
             min-max time:  2.7 - 3.4 sec
         infra time: 0.1
      hmm step time: 3.7
   collapsed 500 queries into 290 clusters with identical naive seqs (0.0 sec)
290 clusters with 10 procs
    prepare_for_hmm: (0.1 sec)
       naive hfrac bounds: 0.015 0.106   (0.105 mean mutation in parameter dir test/new-results/test/parameters/simu/hmm)
    running 10 procs
          read from cache:  naive-seqs 500   logprobs   0
                    calcd:         vtb 118        fwd 160
                   merged:       hfrac  27     lratio  27
             min-max time:  1.4 - 3.8 sec
         infra time: 0.2
      hmm step time: 4.1
236 clusters with 7 procs
    prepare_for_hmm: (0.1 sec)
    running 7 procs
          read from cache:  naive-seqs 629   logprobs 160
                    calcd:         vtb  34        fwd  83
                   merged:       hfrac  12     lratio  16
             min-max time:  0.7 - 4.3 sec
         infra time: 0.1
      hmm step time: 4.5
208 clusters with 5 procs
    prepare_for_hmm: (0.1 sec)
    running 5 procs
          read from cache:  naive-seqs 666   logprobs 243
                    calcd:         vtb  34        fwd  64
                   merged:       hfrac  22     lratio  11
             min-max time:  1.3 - 4.1 sec
         infra time: 0.1
      hmm step time: 4.4
175 clusters with 3 procs
    prepare_for_hmm: (0.1 sec)
    running 3 procs
          read from cache:  naive-seqs 706   logprobs 307
                    calcd:         vtb  17        fwd  53
                   merged:       hfrac  16     lratio   4
             min-max time:  2.9 - 4.0 sec
         infra time: 0.1
      hmm step time: 4.2
155 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 730   logprobs 360
                    calcd:         vtb  27        fwd  46
                   merged:       hfrac  21     lratio   9
             min-max time:  3.9 - 7.3 sec
         infra time: 0.1
      hmm step time: 7.5
125 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 769   logprobs 406
                    calcd:         vtb   8        fwd  29
                   merged:       hfrac   4     lratio   4
             min-max time:  2.7 - 3.2 sec
      hmm step time: 3.4
117 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 779   logprobs 435
                    calcd:         vtb   5        fwd  14
                   merged:       hfrac   4     lratio   2
             min-max time:  1.6 - 2.2 sec
      hmm step time: 2.3
111 clusters with 2 procs
    prepare_for_hmm: (0.1 sec)
    running 2 procs
          read from cache:  naive-seqs 786   logprobs 449
                    calcd:         vtb   1        fwd  13
                   merged:       hfrac   0     lratio   1
             min-max time:  1.4 - 2.1 sec
      hmm step time: 2.3
110 clusters with 1 proc
    prepare_for_hmm: (0.1 sec)
    running 1 proc
          read from cache:  naive-seqs 787   logprobs 462
                    calcd:         vtb   2        fwd  72
                   merged:       hfrac   3     lratio   0
                     time:  8.9 sec
      hmm step time: 9.1
  [93mnote[0m not merging entire cpath history
      loop time: 41.7
getting annotations for final partition
    prepare_for_hmm: (0.1 sec)
    running 10 procs
                    calcd:         vtb 107        fwd   0
             min-max time:  1.1 - 2.1 sec
      hmm step time: 2.3
    read output
  plotting performance (0.3 sec)
        processed 107 hmm output lines with 500 sequences in 107 events  (0 failures)
  --only-csv-plots not implemented for partition plots, so skipping
      total time: 50.7
[92mseed-partition-new-simu[0m          ./bin/partis partition --n-max-queries 500 --persistent-cachefname test/new-results/cache-new-partition.csv --is-simu --sw-cachefname test/new-results/test/parameters/simu/sw-cache.yaml --infname test/new-results/test/simu.yaml --parameter-dir test/new-results/test/parameters/simu --seed-unique-id -237679582940728480 --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/seed-partition-new-simu.yaml
  --n-max-queries: stopped after reading 500 queries from input file
partitioning
smith-waterman
  vsearch: 475 / 500 v annotations (25 failed) with 30 v genes in 0.2 sec
        reading sw results from test/new-results/test/parameters/simu/sw-cache.yaml
      info for 500 / 500 = 1.000   (0 failed, 0 duplicates)
      kept 260 (0.520) unproductive
      removed 431 / 500 = 0.86 sequences with cdr3 length different from seed sequence (leaving 69)
        water time: 0.2
hmm
   collapsed 69 queries into 41 clusters with identical naive seqs (0.0 sec)
41 clusters with 10 procs
    prepare_for_hmm: (0.0 sec)
       naive hfrac bounds: 0.015 0.106   (0.105 mean mutation in parameter dir test/new-results/test/parameters/simu/hmm)
    running 10 procs
          read from cache:  naive-seqs 792   logprobs 534
                    calcd:         vtb  13        fwd   7
                   merged:       hfrac   4     lratio   0
             min-max time:  0.0 - 0.8 sec
      hmm step time: 0.9
46 clusters with 7 procs
    prepare_for_hmm: (0.0 sec)
    running 7 procs
          read from cache:  naive-seqs 805   logprobs 541
                    calcd:         vtb   0        fwd   1
                   merged:       hfrac   0     lratio   0
             min-max time:  0.0 - 0.5 sec
      hmm step time: 0.6
43 clusters with 5 procs
    prepare_for_hmm: (0.0 sec)
    running 5 procs
          read from cache:  naive-seqs 805   logprobs 542
                    calcd:         vtb   0        fwd   3
                   merged:       hfrac   0     lratio   0
             min-max time:  0.0 - 0.5 sec
      hmm step time: 0.6
      removed 62 sequences in unseeded clusters, split 5 seeded clusters into 7 singletons, and merged these into 5 clusters with identical naive seqs
        new n_procs 1 (initial seqs/proc: 6.90   new seqs/proc: 5.00
5 clusters with 1 proc
    prepare_for_hmm: (0.0 sec)
    running 1 proc
          read from cache:  naive-seqs 805   logprobs 545
                    calcd:         vtb   3        fwd   6
                   merged:       hfrac   4     lratio   0
                     time:  0.3 sec
      hmm step time: 0.4
      loop time: 2.5
getting annotations for final partition
    prepare_for_hmm: (0.0 sec)
    running 1 proc
                    calcd:         vtb   1        fwd   0
                     time:  0.2 sec
      hmm step time: 0.2
    read output
        processed 1 hmm output lines with 7 sequences in 1 events  (0 failures)
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
WARNING -237679582940728480 in multiple clusters
      total time: 3.6
[92mvsearch-partition-new-simu[0m       ./bin/partis partition --naive-vsearch --n-max-queries 500 --persistent-cachefname test/new-results/cache-new-partition.csv --is-simu --sw-cachefname test/new-results/test/parameters/simu/sw-cache.yaml --infname test/new-results/test/simu.yaml --parameter-dir test/new-results/test/parameters/simu --seed 1 --n-procs 10 --simulation-germline-dir data/germlines/human --outfname test/new-results/vsearch-partition-new-simu.yaml
  --n-max-queries: stopped after reading 500 queries from input file
partitioning
smith-waterman
  vsearch: 475 / 500 v annotations (25 failed) with 30 v genes in 0.2 sec
        reading sw results from test/new-results/test/parameters/simu/sw-cache.yaml
      info for 500 / 500 = 1.000   (0 failed, 0 duplicates)
      kept 260 (0.520) unproductive
        water time: 0.3
hmm
       naive hfrac bounds: 0.031 0.031   (0.105 mean mutation in parameter dir test/new-results/test/parameters/simu/hmm)
        collapsed 500 sequences into 290 unique naive sequences
    using hfrac bound for vsearch 0.031
    running vsearch 20 times (once for each cdr3 length class): . . . . . . . . . . . . . . . . . . . . 
      vsearch time: 1.2
getting annotations for final partition
    prepare_for_hmm: (0.1 sec)
    running 10 procs
                    calcd:         vtb 124        fwd   0
             min-max time:  1.6 - 2.6 sec
      hmm step time: 2.7
    read output
        processed 124 hmm output lines with 500 sequences in 124 events  (0 failures)
      total time: 4.9
