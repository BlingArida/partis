# ----------------------------------------------------------------------------------------
# was using 25% and talking 5-6 minutes, now at 8.5% and 2.5 minutes (goes down to 0.8% and 2.5 minutes if you clear naive_hfracs_ every time you merge):
#   /home/dralph/work/partis-dev/packages/ham/bcrham --algorithm forward --hmmdir /fh/fast/matsen_e/processed-data/partis/kate-qrs-2016-09-09/Hs-LN1-5RACE-IgG/hmm/hmms --datadir _tmp/gizmo/germline-sets --infile _tmp/gizmo/hmm-9/hmm_input.csv --outfile hmm_output.csv --chain h --random-seed 1475874596 --only-cache-new-vals --input-cachefname _tmp/gizmo/hmm-9/hmm_cached_info.csv --output-cachefname tmp-cache.csv --partition --max-logprob-drop 5.0 --hamming-fraction-bound-lo 0.015 --hamming-fraction-bound-hi 0.08762028181 --logprob-ratio-threshold 18.0 --biggest-naive-seq-cluster-to-calculate 7 --biggest-logprob-cluster-to-calculate 7 --seed-unique-id QB850.405-Vh --ambig-base N

# ----------------------------------------------------------------------------------------
# is best-minus-x stuff still working? (I'm almost certain not -- I'm only calculating the partition log prob the last time through, so all the other times I can't rewind at all. I need to check that this isn't leading to too much over-mergin)
#   - maybe increase --n-partitions-to-write if you get this working?

# need to do allele removal for j (and maybe d?)

# increase default max clusters to calculate for logprob and naive seq

# get ham scons test working again

# add only-big-clusters option

# go through a careful round of partition validation on larger samples

# why are so many sequences in e.g. /fh/fast/matsen_e/data/kate-qrs-2016-09-09/processed_data/Hs-LN1-5RACE-IgK/03_fastx_out/Hs-LN1-5RACE-IgK.trimmed.noN.collapsed.fasta unproductive?

# purity/completeness should take account of the fact that we remove duplicate sequences (and simulation should probably just forbid/remove duplicates) UPDATE oh wait but we're not removing duplicates in simulation

# 

# bryan's super-short-read sample
#  - better insertion mutation rate?
#  - print cdr3 mutation rate (and maybe also make plots without dividing by length?)

# ----------------------------------------------------------------------------------------
# print_reco_event:
#    - maybe simulation line shouldn't be associated with a particular inferred line (see below)
#    - fix multiple-indel printing'
#    - just print the length for large fv/jf insertions

# testing:
#  - larger sample sizes
#  - at least make sure the seed cluster is big
#  - move testing off --only-genes
#  - bigger n-max-cluster-to-calc values
#  - can probably remove data stuff (UPDATE no, need to test .fa treatment and some code blocks -- but it can be a very small number of sequences)

# germline set  generation:
#     all alleles (including new ones) should have roughly the same mut freq distribution (especially at low mutation)'
#     collapse at least the largest clones before inferring new alleles'
#     finish switch from fitfo to self.fitfos'
#     try trevor's k-means-style-ish clustering idea
#     maybe:
#       y-icpt (i.e. allele prevalence) could also go into the decision about whether to remove the template gene or not'
#       add requirement that all positions in multiple-snp alleles are correlated (?)'
#       do all the plots separately for each potential original snp base (i.e. separate plot for A, C, G, T)?'
#       add requirement for mulitple j genes for new alleles (?)'
#       increase n_max_mutations_per_segment for highly-mutated repertoires (?)'

# optimization
#  - go through glomerator (and maybe dphandler) and make sure everything that should be a reference or a pointer is one
#  - switch all the c++ map.count() calls to map.find()
#  - switch only_genes in c++ to a set
#  - may be worthwhile to switch from colon-string hash map scheme to something shorter
#  - kbounds logical or should check if/handle case where one of them is unset (ok not really an optimization, but it's not very important atm)
#  - i think i could switch to running all the ksets of each gene at once (which would drastically reduce the dphandler trellis cache size, since we'd only need to keep one gene at a time), but this would involve a lot of careful rewriting in dphandler

# maybe
#  - make padding specific to each cdr3 subclass UPDATE maybe not worthwhile? doesn't seem to be an exorbitant amount of padding any more'
# lower priority
#  - large cluster annotations seem to be way too willing to use ridiculously long insertions (e.g. in /fh/fast/matsen_e/processed-data/partis/kate-qrs-2016-09-09/seeds/QB850.402-Vh/Hs-LN1-5RACE-IgG-cluster-annotations.csv)
