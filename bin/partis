#!/usr/bin/env python
from distutils.version import StrictVersion
import argparse
import copy
import time
import random
import sys
import subprocess
import multiprocessing
import numpy
import scipy
if StrictVersion(scipy.__version__) < StrictVersion('0.17.0'):
    raise RuntimeError("scipy version 0.17 or later is required (found version %s)." % scipy.__version__)
import colored_traceback.always
import os
partis_dir = os.path.dirname(os.path.realpath(__file__)).replace('/bin', '')
if not os.path.exists(partis_dir):
    print 'WARNING current script dir %s doesn\'t exist, so python path may not be correctly set' % partis_dir
sys.path.insert(1, partis_dir + '/python')
sys.path.insert(1, partis_dir + '/packages/baltic')

import utils
import glutils
import processargs
import seqfileopener
from partitiondriver import PartitionDriver

# ----------------------------------------------------------------------------------------
def default_parameter_dir(args):
    if args.infname is not None:
        label = args.infname[ : args.infname.rfind('.')]  # probably ok not to use the full path here
        label = label.replace('/', '_')
    else:
        label = 'xxx-dummy-xxx'  # gah, this is a terrible way to do this. But I can't set it to None, and I need some way to communicate that it isn't a valid pdir, so...
    pdir = '_output/' + label
    return pdir

# ----------------------------------------------------------------------------------------
def set_default_parameter_dir(args):
    if args.parameter_dir is None and not args.rearrange_from_scratch and not args.mutate_from_scratch:
        args.parameter_dir = default_parameter_dir(args)

# ----------------------------------------------------------------------------------------
def run_simulation(args):
    from recombinator import Recombinator

    if args.batch_system is not None and args.n_procs > 1 and not args.subsimproc:
        print '  %s setting subsimproc' % utils.color('red', 'warning')
        args.subsimproc = True
    if args.n_trees is None:
        args.n_trees = max(1, int(float(args.n_sim_events) / args.n_procs))
    if args.outfname is None:
        raise Exception('have to specify --outfname for simulation')
    if args.n_max_queries != -1:
        print '  note: --n-max-queries is not used when simulating (use --n-sim-events to set the simulated number of rearrangemt events)'

    if args.parameter_dir is None:
        if not args.rearrange_from_scratch and not args.mutate_from_scratch:
            raise Exception('either --parameter-dir must be specified, or one of the scratch simulation options')
        if args.mutate_from_scratch and not args.rearrange_from_scratch:
            raise Exception('haven\'t yet implemented mutating from scratch without rearranging from scratch')  # and maybe not ever
    else:
        if args.rearrange_from_scratch or args.mutate_from_scratch or args.generate_germline_set:
            raise Exception('you can\'t specify --parameter-dir if you also set either of the scratch options or --generate-germline-set')
        if args.initial_germline_dir is not None:
            raise Exception('you can\'t specify both --parameter-dir and --initial-germline-dir (germline info has to be read from the parameter dir)')

    utils.prep_dir(args.workdir)

    default_prevalence_fname = args.workdir + '/allele-prevalence.csv'
    if args.generate_germline_set and args.allele_prevalence_fname is None:
        args.allele_prevalence_fname = default_prevalence_fname

    if args.initial_germline_dir is None:  # if the gl info dir wasn't explicitly set on the command line, we have to decide what to use
        if args.parameter_dir is not None:  # if --parameter-dir was explicitly set, assume there's gl info in this --parameter-dir
            input_gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
        else:  # otherwise use the default
            input_gldir = args.default_initial_germline_dir
    else:
        input_gldir = args.initial_germline_dir
    assert len(args.loci) == 1  # needs to be implemented
    glfo = glutils.read_glfo(input_gldir, args.locus, only_genes=args.only_genes)
    if not args.im_a_subproc and args.rearrange_from_scratch and args.generate_germline_set:
        glutils.generate_germline_set(glfo, args.n_genes_per_region, args.n_sim_alleles_per_gene, args.min_sim_allele_prevalence_freq, args.allele_prevalence_fname)  # NOTE removes unwanted genes from <glfo>
        print '  writing generated germline set to %s/' % args.outfname.replace('.csv', '-glfo')
        glutils.write_glfo(args.outfname.replace('.csv', '-glfo'), glfo)
    if args.subsimproc:
        working_gldir = args.workdir + '/' + glutils.glfo_dir
        glutils.write_glfo(working_gldir, glfo)

    # ----------------------------------------------------------------------------------------
    def get_subproc_cmd_str(n_events, iproc, workdir, outfname):
        clist = copy.deepcopy(sys.argv)
        utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
        utils.remove_from_arglist(clist, '--subsimproc')
        clist.append('--im-a-subproc')
        utils.replace_in_arglist(clist, '--seed', str(args.seed + iproc))
        utils.replace_in_arglist(clist, '--workdir', workdir)
        utils.replace_in_arglist(clist, '--outfname', outfname)
        utils.replace_in_arglist(clist, '--n-sim-events', str(n_events))
        utils.replace_in_arglist(clist, '--initial-germline-dir', working_gldir)
        if args.allele_prevalence_fname is not None:
            utils.replace_in_arglist(clist, '--allele-prevalence-fname', args.allele_prevalence_fname)
        return ' '.join(clist)

    # ----------------------------------------------------------------------------------------
    def make_events(n_events, iproc, workdir, outfname, random_ints):
        reco = Recombinator(args, glfo, seed=args.seed+iproc, workdir=workdir, outfname=outfname)
        for ievt in range(n_events):
            reco.combine(random_ints[ievt])
        # reco.print_validation_values()

    def get_workdir(iproc):
        return args.workdir + '/sub-' + str(iproc)
    def get_outfname(iproc):
        return get_workdir(iproc) + '/' + os.path.basename(args.outfname)

    if not args.im_a_subproc:
        print 'simulating'

    set_default_parameter_dir(args)

    n_per_proc = int(float(args.n_sim_events) / args.n_procs)

    # generate all the random seeds NOTE these aren't used if <args.subsimproc> is set, i.e. results will be different with and without <args.subsimproc> set.
    all_random_ints = []
    for iproc in range(args.n_procs):  # have to do it all at once, 'cause each of the subprocesses is going to reset its seed and god knows what happens to our seed at that point
        all_random_ints.append([random.randint(0, numpy.iinfo(numpy.int32).max) for i in range(n_per_proc)])

    if args.n_procs == 1:  # multiprocessing is kind of messy
        make_events(n_per_proc, 0, args.workdir, args.outfname, all_random_ints[0])
    else:  # start the processes and wait for 'em to finish
        cmdfos = [{'cmd_str' : get_subproc_cmd_str(n_per_proc, iproc, get_workdir(iproc), get_outfname(iproc)) if args.subsimproc else None,
                   'workdir' : get_workdir(iproc),
                   'logdir' : args.workdir + '/log-' + str(iproc),  # have to be different than <workdirs> since ./bin/partis barfs if its workdir already exists (as it should)
                   'outfname' : get_outfname(iproc)}
                  for iproc in range(args.n_procs)]
        if args.subsimproc:
            utils.run_cmds(cmdfos, batch_system=args.batch_system, batch_options=args.batch_options, batch_config_fname=args.batch_config_fname, debug='print')
        else:
            for iproc in range(args.n_procs):
                proc = multiprocessing.Process(target=make_events, args=(n_per_proc, iproc, cmdfos[iproc]['workdir'], cmdfos[iproc]['outfname'], all_random_ints[iproc]))
                proc.start()
            while len(multiprocessing.active_children()) > 0:
                sys.stdout.flush()
                time.sleep(1)

        # check and merge output
        n_total_events = 0
        for iproc in range(args.n_procs):
            fname = cmdfos[iproc]['outfname']
            if not os.path.exists(fname):
                raise Exception('output simulation file %s d.n.e.' % fname)
            n_events = int(subprocess.check_output('grep -v unique_id ' + fname + ' | awk -F, \'{print $2}\' | uniq | wc -l', shell=True).split()[0])  # if *adjacent* events have the same rearrangement parameters (and hence reco id), they'll show up as the same event. It has happened!
            n_total_events += n_events
            if n_events < n_per_proc - 3:  # give ourselves a little breathing room for adjacent events that have the same rearrangement parameters (it's only happened once, ever, so maybe 3 is enough?)
                raise Exception('only found %d events (expected %d) in output file %s' % (n_events, n_per_proc, fname))
        utils.merge_csvs(args.outfname, [cmdfos[i]['outfname'] for i in range(args.n_procs)])
        print '   read %d events from %d files' % (n_total_events, args.n_procs)

    if not args.im_a_subproc and args.allele_prevalence_fname is not None:  # check final prevalence freqs
        glutils.check_allele_prevalence_freqs(args.outfname, glfo, args.allele_prevalence_fname, only_region='v')
        if args.allele_prevalence_fname == default_prevalence_fname:
            os.remove(default_prevalence_fname)

    if args.subsimproc:
        glutils.remove_glfo_files(working_gldir, args.locus)
        for iproc in range(args.n_procs):
            os.rmdir(cmdfos[iproc]['logdir'])

    if not args.im_a_subproc:
        try:
            os.rmdir(args.workdir)
        except OSError:
            raise Exception('workdir (%s) not empty: %s' % (args.workdir, ' '.join(os.listdir(args.workdir))))  # hm... you get weird recursive exceptions if you get here. Oh, well, it still works

# ----------------------------------------------------------------------------------------
def get_glfos(args, actions):
    if actions[0] == 'cache-parameters':  # if we're caching parameters, read initial gl info from <args.initial_germline_dir> (then the final glfo gets written to the parameter dir)
        gldir = args.default_initial_germline_dir if args.initial_germline_dir is None else args.initial_germline_dir
    elif 'xxx-dummy-xxx' in args.parameter_dir:  # terrible hack to indicate we're dealing with existing output
        if args.initial_germline_dir is not None:  # if an explicit dir was set on the command line, use it
            gldir = args.initial_germline_dir
        else:  # otherwise hope the default one (probably in data/germlines/human is good enough)
            gldir = args.default_initial_germline_dir
    else:
        gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir

    glfo = glutils.read_glfo(gldir, locus=args.locus, only_genes=args.only_genes)
    simglfo = None
    if not args.is_data:
        if args.simulation_germline_dir is not None:  # if an explicit dir was set on the command line
            simglfo = glutils.read_glfo(args.simulation_germline_dir, locus=args.locus)  # probably don't apply <args.only_genes> (?)
        else:
            simglfo = glfo  # TODO it would be nicer, in the future, to store the simulation germline info in the simulation file rather than doing this
    return glfo, simglfo

# ----------------------------------------------------------------------------------------
def read_input_file(args, simglfo):
    if args.infname is None:
        return None, None
    input_info, reco_info = seqfileopener.get_seqfile_info(args.infname, args.is_data, n_max_queries=args.n_max_queries, args=args, simglfo=simglfo)
    if len(input_info) > 1000 and args.n_procs == 1:
        print '  note: running on %d sequences with only %d processes. This will be kinda slow, so it might be a good idea to set --n-procs N to the number of processors on your local machine, or look into non-local parallelization with --batch-system.\n' % (len(input_info), args.n_procs)
    return input_info, reco_info

# ----------------------------------------------------------------------------------------
def run_partitiondriver(args):
    set_default_parameter_dir(args)

    actions = [args.action]  # do *not* use <args.action> after this
    if args.action in ['annotate', 'partition'] and not os.path.exists(args.parameter_dir):
        actions = ['cache-parameters'] + actions
        print '  parameter dir \'%s\' does not exist, so caching a new set of parameters before running action \'%s\'' % (args.parameter_dir, actions[1])

    glfo, simglfo = get_glfos(args, actions)
    input_info, reco_info = read_input_file(args, simglfo)
    parter = PartitionDriver(args, glfo, input_info, simglfo, reco_info)
    parter.run(actions)
    parter.clean()

# ----------------------------------------------------------------------------------------
class MultiplyInheritedFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass
formatter_class = MultiplyInheritedFormatter
parser = argparse.ArgumentParser(formatter_class=MultiplyInheritedFormatter)
subparsers = parser.add_subparsers(dest='action')

parent_parser = argparse.ArgumentParser(add_help=False)

parent_parser.add_argument('--locus', default='igh', choices=utils.loci.keys(), help='which immunoglobulin or t-cell receptor locus?')
parent_parser.add_argument('--loci', help='which immunoglobulin or t-cell receptor loci?')
parent_parser.add_argument('--chain', choices=('h', 'k', 'l'), help='DEPRECATED in favor of --locus, exists only for backwards compatibility')
parent_parser.add_argument('--species', default='human', choices=('human', 'mouse'), help='Which species?')
parent_parser.add_argument('--queries', help='Colon-separated list of query names to which to restrict the analysis')
parent_parser.add_argument('--queries-to-include', help='when using --n-random-queries, look for, and include, these additional uids (in contast to --queries, which includes *only* the indicated uids) NOTE *not* compatible with --n-max-queries, i.e. use --n-random-queries instead')
parent_parser.add_argument('--reco-ids', help='Colon-separated list of rearrangement-event IDs to which we restrict ourselves')  # or recombination events
parent_parser.add_argument('--n-max-queries', type=int, default=-1, help='Maximum number of query sequences to read, starting from beginning of input file')
parent_parser.add_argument('--n-random-queries', type=int, help='choose this many queries at random from input file')
parent_parser.add_argument('--istartstop', help='colon-separated start:stop line indices for input sequence file (with python slice conventions, e.g. if set to \'2:4\' will skip the zeroth and first sequences, and then take the following two sequences, and then skip all subsequence sequences). Applied before any other input filters, e.g. --n-max-queries, --queries, --reco-ids, etc.')

parent_parser.add_argument('--debug', type=int, default=0, choices=[0, 1, 2], help='Debug verbosity level.')
parent_parser.add_argument('--sw-debug', type=int, choices=[0, 1, 2], help='Debug level for Smith-Waterman.')
parent_parser.add_argument('--abbreviate', action='store_true', help='Abbreviate/translate sequence ids to improve readability of partition debug output. Uses a, b, c, ..., aa, ab, ...')
parent_parser.add_argument('--print-git-commit', action='store_true', help='print sys.argv, git commit hash, and tag info')

parent_parser.add_argument('--n-procs', default='1', help='Number of processes over which to parallelize (Can be colon-separated list: first number is procs for hmm, second (should be smaller) is procs for smith-waterman)')
parent_parser.add_argument('--n-max-procs', default=100, help='Never allow more processes than this (default %(default)d)')
parent_parser.add_argument('--n-max-to-calc-per-process', default=250, help='if a bcrham process calc\'d more than this many fwd + vtb values (and this is the first time with this number of procs), don\'t decrease the number of processes in the next step (default %(default)d)')
parent_parser.add_argument('--min-hmm-step-time', default=2., help='if a clustering step takes fewer than this many seconds, always reduce n_procs')
parent_parser.add_argument('--batch-system', choices=['slurm', 'sge'], help='batch system with which to attempt paralellization')
parent_parser.add_argument('--batch-options', help='additional options to apply to --batch-system (e.g. --batch-options="--foo bar")')
parent_parser.add_argument('--batch-config-fname', default='/etc/slurm-llnl/slurm.conf', help='system-wide batch system configuration file name')  # for when you're running the whole thing within one slurm allocation, i.e. with  % salloc --nodes N ./bin/partis [...]

parent_parser.add_argument('--only-smith-waterman', action='store_true', help='Exit after finishing smith-waterman.')
parent_parser.add_argument('--count-parameters', action='store_true', help='force parameter counting when action is not cache-parameters (presumably so that you can plot them)')
parent_parser.add_argument('--dont-write-parameters', action='store_true', help='don\'t write parameters to disk even if you\'ve counted them (mostly for use in conjunction with --only-smith-waterman, so you can avoid cluttering up your file system)')
parent_parser.add_argument('--write-trimmed-and-padded-seqs-to-sw-cachefname', action='store_true', help='after running sw, trim and pad sequences *before* writing to the sw cache file (rather than after). Note that this will in some cases cause waterer.py to not be able to read sw results from this cache file.')
parent_parser.add_argument('--partis-dir', default=partis_dir, help='for internal use only')
parent_parser.add_argument('--ig-sw-binary', default=partis_dir + '/packages/ig-sw/src/ig_align/ig-sw', help='Path to ig-sw executable.')
parent_parser.add_argument('--vsearch-binary',  help='Path to vsearch binary (vsearch binaries for linux and darwin are pre-installed in bin/, but for other systems you need to get your own)')
parent_parser.add_argument('--is-simu', action='store_true', help='Set if running on simulated sequences')
parent_parser.add_argument('--skip-unproductive', action='store_true', help='Skip sequences which Smith-Waterman determines to be unproductive (i.e. if they have stop codons, out of frame cdr3, or mutated cyst/tryp/phen)')
parent_parser.add_argument('--also-remove-duplicate-sequences-with-different-lengths', action='store_true', help='By default we remove any queries which have exactly the same sequence as a previous query. If this is set, we also consider as duplicates sequences which are sub/super strings of previous sequences (we keep the longest one).')
parent_parser.add_argument('--dont-remove-framework-insertions', action='store_true', help='By default we trim anything to the 5\' of V and 3\' of J in order to remove queries with identical coding regions. This turns that off.')
parent_parser.add_argument('--dont-rescale-emissions', action='store_true', help='Don\'t scale each hmm\'s emission probabilities to account for the branch length of each individual sequence.')
parent_parser.add_argument('--no-indels', action='store_true', help='Tell smith-waterman not to look for indels, by drastically increasing the gap-open penalty (you can also set the penalty directly).')
parent_parser.add_argument('--seed', type=int, default=int(time.time()), help='Random seed for use (mostly) by recombinator (to allow reproducibility)')
parent_parser.add_argument('--min-observations-to-write', type=int, default=20, help='When writing hmm model files, if we see a gene version fewer times than this, we average over other alleles, or other primary versions, etc. (see hmmwriter). NOTE default is manipulated in partitiondriver.py')
parent_parser.add_argument('--no-per-base-mfreqs', action='store_true')
parent_parser.add_argument('--region-end-exclusion-length', type=int, default=0, help='when counting/writing parameters, ignore this many bases abutting non-templated insertions for calculating mutation frequencies (note: doesn\'t make a difference (hence set to 0 by default) probably because we\'re setting a strongish prior on these bases when writing hmms anyway')

parent_parser.add_argument('--only-genes', help='Colon-separated list of genes to which to restrict the analysis. If any regions (V/D/J) are not represented among these genes, these regions are left unrestricted.')
parent_parser.add_argument('--n-max-per-region', default='3:5:2', help='Number of best smith-waterman matches (per region, in the format v:d:j) to pass on to the hmm')
parent_parser.add_argument('--gap-open-penalty', type=int, default=30, help='Penalty for indel creation in Smith-Waterman step.')
parent_parser.add_argument('--max-vj-mut-freq', type=float, default=0.4, help='skip sequences whose mutation rates in V and J are greater than this (it\'s really not possible to get meaningful smith-waterman matches above this)')
parent_parser.add_argument('--max-logprob-drop', type=float, default=5., help='stop glomerating when the total logprob has dropped by this much')
parent_parser.add_argument('--n-simultaneous-seqs', type=int, help='Number of simultaneous sequences on which to run the multi-HMM (e.g. 2 for a pair hmm)')
parent_parser.add_argument('--simultaneous-true-clonal-seqs', action='store_true', help='Run true clonal sequences together simultaneously with the multi-HMM.')

parent_parser.add_argument('--infname', help='input sequence file in .fa, .fq or .csv (if .csv, specify id string and sequence headers with --name-column and --seq-column)')
parent_parser.add_argument('--name-column', help='csv column name for sequence ids')
parent_parser.add_argument('--seq-column', help='csv column name for nucleotide sequences')
parent_parser.add_argument('--outfname', help='output file name')
parent_parser.add_argument('--presto-output', action='store_true', help='write output file in presto format')
parent_parser.add_argument('--extra-annotation-columns', help='Extra columns to add to the the (fairly minimal) set of information written by default to annotation output files (choose from: %s' % ' '.join(utils.extra_annotation_headers))  # NOTE '-columns' in command line arg, but '-headers' in utils (it's more consistent that way, I swear)
parent_parser.add_argument('--linearham', action='store_true', help='write hmm input file in linearham format')
parent_parser.add_argument('--cluster-annotation-fname', help='output file for cluster annotations (default is <--outfname>-cluster-annotations.csv)')
parent_parser.add_argument('--parameter-dir', help='Directory to/from which to write/read sample-specific parameters. If not specified, a default location is used (and printed to std out). If it does not exist, we infer parameters before proceeding to the desired action.')
parent_parser.add_argument('--parameter-type', default='hmm', choices=('sw', 'hmm'), help='Use parameters from Smith-Waterman (sw) or the HMM (hmm) subdirectories for inference/simulation? (you should almost certainly use the hmm ones, but sw is occasionally useful for debugging)')
parent_parser.add_argument('--persistent-cachefname', help='Name of file which will be used as an initial cache file (if it exists), and to which all cached info will be written out before exiting.')
parent_parser.add_argument('--sw-cachefname', help='Smith-Waterman cache file name. Default is set using a hash of all the input sequence ids (in partitiondriver, since we have to read the input file first).')
parent_parser.add_argument('--workdir', help='Temporary working directory (default is set below)')

parent_parser.add_argument('--plotdir', help='Base directory to which to write plots (by default this is not set, and consequently no plots are written')
parent_parser.add_argument('--plot-performance', action='store_true', help='renamed to --plot-annotation-performance')
parent_parser.add_argument('--plot-annotation-performance', action='store_true', help='Write out plots comparing true and inferred annotation accuracy distributions')
parent_parser.add_argument('--only-csv-plots', action='store_true', help='only write csv plots (svg writing is pretty slow, and it\'s frequently better to make comparison plots from csvs with ./bin/compare-plotdirs.py anyway)')
parent_parser.add_argument('--only-overall-plots', action='store_true', help='don\'t write per-gene plots (it\'s kind of slow writing that many plots)')

parent_parser.add_argument('--default-initial-germline-dir', default=partis_dir + '/data/germlines', help='For internal use only. To specify your own germline directory from which to start, use --initial-germline-dir instead.')
parent_parser.add_argument('--initial-germline-dir', help='Directory with fastas from which to read germline set. Only used when caching parameters, during which its contents is copied into --parameter-dir, perhaps (i.e. if specified) with modification. NOTE default is set below, because control flow is too complicated for argparse')
parent_parser.add_argument('--simulation-germline-dir', help='Germline directory that was used for simulation (used if --is-simu is set, altough if the default germline info is similar to that used for simulation it may not be necessary)')
parent_parser.add_argument('--aligned-germline-fname', help='fasta file with alignments for each V gene')

parent_parser.add_argument('--n-alleles-per-gene', type=int, default=None, help='number of alleles to assume per gene when removing spurious alleles during parameter caching (default is set below -- 1 if looking for new alleles, otherwise 2) NOTE difference to --n-sim-alleles-per-gene')
parent_parser.add_argument('--min-allele-prevalence-fraction', type=float, default=0.0005, help='Remove any alleles that represent less than this fraction of the repertoire.')
parent_parser.add_argument('--n-max-total-alleles', type=int, help='maximum number of alleles per segment to allow when removing alleles (presumably just used for testing).')

parent_parser.add_argument('--leave-default-germline', action='store_true')
parent_parser.add_argument('--dont-remove-unlikely-alleles', action='store_true')
parent_parser.add_argument('--allele-cluster', action='store_true')
parent_parser.add_argument('--kmeans-allele-cluster', action='store_true')
parent_parser.add_argument('--dont-find-new-alleles', action='store_true')
# parent_parser.add_argument('--always-find-new-alleles', action='store_true', help='By default we only look for new alleles if a repertoire\'s mutation rate is amenable to reasonable new-allele sensitivity (i.e. if it\'s not crazy high). This overrides that.')
parent_parser.add_argument('--debug-allele-finding', action='store_true', help='print lots of debug info on new-allele fits')
parent_parser.add_argument('--new-allele-fname', help='fasta fname to which to write any new alleles (they are also written, together with previously-known alleles that are also in the sample, to --parameter-dir)')
parent_parser.add_argument('--n-max-allele-finding-iterations', type=int, default=1, help='maximum number of times to look for new alleles')
parent_parser.add_argument('--n-max-new-alleles-per-gene-per-iteration', type=int, default=3, help='with an option name that long...')
parent_parser.add_argument('--n-max-snps', type=int, default=8, help='when new-allele finding, look for new alleles separated from existing alleles by up to this many SNPs')
parent_parser.add_argument('--n-max-mutations-per-segment', type=int, default=23, help='when new-allele finding, exclude sequences which have more than this many mutations in the V segment')
parent_parser.add_argument('--min-allele-finding-gene-length', type=int, default=150, help='if (after excluding particularly short reads) the reads for a gene are shorter than this, then don\'t look for new alleles with/on this gene')
parent_parser.add_argument('--plot-and-fit-absolutely-everything', type=int, help='fit every single position for this <istart> and write every single corresponding plot (slow as hell, and only for debugging/making plots for paper)')
parent_parser.add_argument('--cluster-indices', help='indices of clusters (when sorted largest to smallest) to print for --view-annotations and --view-partitions')

parent_parser.add_argument('--pre-vsearch', action='store_true')

# ----------------------------------------------------------------------------------------
subconfig = {
    'version'          : {'func' : int, 'help' : 'print version information and exit'},
    'cache-parameters' : {'func' : run_partitiondriver, 'help' : 'Cache parameter values and write hmm model files.'},
    'annotate'         : {'func' : run_partitiondriver, 'help' : 'Annotate sequences in input file, i.e. run the viterbi algorithm, using pre-existing parameter directory.'},
    'partition'        : {'func' : run_partitiondriver, 'help' : 'Partition sequences in input file into clonally-related families using pre-existing parameter directory.'},
    'simulate'         : {'func' : run_simulation,      'help' : 'Generate simulated sequences based on information in pre-existing parameter directory.'},
    'view-annotations' : {'func' : run_partitiondriver, 'help' : 'Print (to std out) the annotations from an existing annotation output csv.'},
    'view-partitions'  : {'func' : run_partitiondriver, 'help' : 'Print (to std out) the partitions from an existing partition output csv.'},
    'view-cluster-annotations'    : {'func' : run_partitiondriver, 'help' : 'Print (to std out) the partitions, and annotations for the best partition, from an existing partition output csv.'},
    'view-alternative-naive-seqs' : {'func' : run_partitiondriver, 'help' : 'Print (to std out) a comparison of the naive sequences corresponding to sub- and super-clusters of the cluster specified with --queries. You must have specified --calculate-alternative-naive-seqs (or --persistent-cachefname) in a previous partition step so that this information was saved.'},
    'plot-partitions'  : {'func' : run_partitiondriver, 'help' : 'Plot existing partitions and cluster annotations.'},
    'run-viterbi'      : {'func' : run_partitiondriver, 'help' : 'deprecated, do not use'},
}

subargs = {subname : [] for subname in subconfig}

subargs['partition'].append({'name' : '--naive-hamming', 'kwargs' : {'action' : 'store_true', 'help' : 'agglomerate purely with naive hamming distance, i.e. set the low and high preclustering bounds to the same value'}})
subargs['partition'].append({'name' : '--naive-vsearch', 'kwargs' : {'action' : 'store_true', 'help' : 'Very fast clustering: infer naive (unmutated ancestor) for each input sequence, then toss it all into vsearch. But, of course, not as accurate as the slower methods.'}})
subargs['partition'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'Throw out all sequences that are not clonally related to this sequence id. Much much much faster than partitioning the entire sample (well, unless your whole sample is one family).'}})
subargs['partition'].append({'name' : '--seed-seq', 'kwargs' : {'help' : 'same effect as --seed-unique-id, but specifies the sequence instead of that sequence\'s id (so that it doesn\'t have to be in the original input file)'}})
subargs['partition'].append({'name' : '--random-seed-seq', 'kwargs' : {'action' : 'store_true', 'help' : 'choose a sequence at random from the input file, and use it as the seed for seed partitioning (as if it had been set as the --seed-unique-id)'}})
subargs['partition'].append({'name' : '--annotation-clustering', 'kwargs' : {'help' : 'Perform annotation-based clustering: group together sequences with the same V and J, same CDR3 length, and 90%% cdr identity. Very, very inaccurate.'}})
subargs['partition'].append({'name' : '--annotation-clustering-thresholds', 'kwargs' : {'default' : '0.9', 'help' : 'colon-separated list of thresholds for annotation-based (e.g. vollmers) clustering'}})
subargs['partition'].append({'name' : '--print-cluster-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'deprecated'}})
subargs['partition'].append({'name' : '--naive-hamming-bounds', 'kwargs' : {'help' : 'Clustering bounds (lo:hi colon-separated pair) on naive sequence hamming distance. If not specified, the bounds are set based on the per-dataset mutation levels. For most purposes should be left at the defaults.'}})
subargs['partition'].append({'name' : '--logprob-ratio-threshold', 'kwargs' : {'type' : float, 'default' : 18., 'help' : 'reaches a min value of <this> minus five for large clusters.'}})
subargs['partition'].append({'name' : '--synthetic-distance-based-partition', 'kwargs' : {'action' : 'store_true', 'help' : 'Use simulation truth info to create a synthetic distance-based partition (for validation).'}})
subargs['partition'].append({'name' : '--cache-naive-hfracs', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to naive sequences and log probabilities, also cache naive hamming fractions between cluster pairs. Only really useful for plotting or testing.'}})
subargs['partition'].append({'name' : '--n-precache-procs', 'kwargs' : {'type' : int, 'help' : 'Number of processes to use when precaching naive sequences. Default is set based on some heuristics, and should typically only be overridden for testing.'}})
subargs['partition'].append({'name' : '--biggest-naive-seq-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--biggest-logprob-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--n-partitions-to-write', 'kwargs' : {'type' : int, 'default' : 10, 'help' : 'Number of partitions (surrounding the best partition) to write to output file.'}})
subargs['partition'].append({'name' : '--naive-swarm', 'kwargs' : {'action' : 'store_true', 'help' : 'Use swarm instead of vsearch, which the developer recommends. Didn\'t seem to help much, and needs more work to optimize threshold, so DO NOT USE.'}})
subargs['partition'].append({'name' : '--small-clusters-to-ignore', 'kwargs' : {'help' : 'colon-separated list (or dash-separated inclusive range) of cluster sizes to throw out after several partition steps. E.g. \'1:2\' will, after <--n-steps-at-which-to-ignore-small-clusters> partition steps, throw out all singletons and pairs. Alternatively, \'1-10\' will ignore all clusters with size less than 11.'}})
subargs['partition'].append({'name' : '--n-steps-after-which-to-ignore-small-clusters', 'kwargs' : {'type' : int, 'default' : 3, 'help' : 'number of partition steps after which to throw out small clusters (where "small" is controlled by <--small-clusters-to-ignore>). (They\'re thrown out before this if we get to n_procs one before this).'}})
subargs['partition'].append({'name' : '--n-final-clusters', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and there are still more than this many clusters, attempt to keep merging until there aren\'t. NOTE should\'ve called this --n-max-final-clusters, but too late to change now'}})
subargs['partition'].append({'name' : '--min-largest-cluster-size', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and the largest cluster isn\'t this big, attempt to keep merging until it is.'}})
subargs['partition'].append({'name' : '--calculate-alternative-naive-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'write to disk all the information necessary to, in a later step, print alternative inferred naive sequences (i.e. visualize uncertainty in the inferred naive sequence). All this really does is set --persistent-cachefname, i.e. copy the hmm cache file that we would anyway be making (but deleting) to somewhere sensible for later use.'}})
subargs['partition'].append({'name' : '--max-cluster-size', 'kwargs' : {'type' : int, 'help' : 'stop clustering immediately if any cluster grows larger than this (useful for limiting memory usage, which can become a problem when the final partition contains very large clusters)'}})
subargs['partition'].append({'name' : '--write-additional-cluster-annotations', 'kwargs' : {'help' : 'in addition to writing annotations for each cluster in the best partition, also write annotations for several partitions on either side of the best partition. Specified as a pair of numbers \'m:n\' for m partitions before, and n partitions after, the best partition.'}})

subargs['simulate'].append({'name' : '--mutation-multiplier', 'kwargs' : {'type' : float, 'help' : 'Multiply observed branch lengths by some factor when simulating, e.g. if in data it was 0.05, but you want closer to ten percent in your simulation, set this to 2'}})
subargs['simulate'].append({'name' : '--mimic-data-read-length', 'kwargs' : {'action' : 'store_true', 'help' : 'trim V 5\' and D 3\' to mimic read lengths seen in data'}})
subargs['simulate'].append({'name' : '--n-sim-events', 'kwargs' : {'type' : int, 'default' : 1, 'help' : 'Number of rearrangement events to simulate'}})
subargs['simulate'].append({'name' : '--n-trees', 'kwargs' : {'type' : int, 'help' : 'Number of phylogenetic trees from which to choose during simulation (we pre-generate this many trees before starting a simulation run, then for each rearrangement event choose one at random -- so this should be at least of order the number of simulated events, so your clonal families don\'t all have the same tree).'}})
subargs['simulate'].append({'name' : '--n-leaves', 'kwargs' : {'type' : float, 'default' : 5., 'help' : 'Parameter describing the number of leaves per tree (maybe the mean, maybe not -- depends on the distribution)'}})
subargs['simulate'].append({'name' : '--constant-number-of-leaves', 'kwargs' : {'action' : 'store_true', 'help' : 'Give all trees the same number of leaves'}})
subargs['simulate'].append({'name' : '--n-leaf-distribution', 'kwargs' : {'default' : 'geometric', 'choices' : ['geometric', 'box', 'zipf'], 'help' : 'distribution from which to draw the number of leaves for each tree'}})
subargs['simulate'].append({'name' : '--indel-frequency', 'kwargs' : {'default' : 0., 'type' : float, 'help' : 'fraction of simulated sequences with indels'}})
subargs['simulate'].append({'name' : '--mean-indels-per-indeld-seq', 'kwargs' : {'default' : 1.2, 'type' : float, 'help' : 'mean number of indels in each sequence which we\'ve already decided has indels (geometric distribution)'}})
subargs['simulate'].append({'name' : '--mean-indel-length', 'kwargs' : {'default' : 5, 'help' : 'mean length of each indel (geometric distribution)'}})
subargs['simulate'].append({'name' : '--indel-location', 'kwargs' : {'choices' : [None, 'v', 'cdr3'], 'help' : 'where to put the indels'}})
# NOTE command to generate gtr parameter file: [stoat] partis/ > zcat /shared/silo_researcher/Matsen_F/MatsenGrp/data/bcr/output_sw/A/04-A-M_gtr_tr-qi-gi.json.gz | jq .independentParameters | grep -v '[{}]' | sed 's/["\:,]//g' | sed 's/^[ ][ ]*//' | sed 's/ /,/' | sort >data/gtr.txt)
subargs['simulate'].append({'name' : '--gtrfname', 'kwargs' : {'default' : partis_dir + '/data/recombinator/gtr.txt', 'help' : 'File with list of GTR parameters. Fed into bppseqgen along with the chosen tree. Corresponds to an arbitrary dataset at the moment, but eventually will be inferred per-dataset.'}})
subargs['simulate'].append({'name' : '--rearrange-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for rearrangement-level parameters, and instead make up some plausible stuff from scratch (unless --mutate-from-scratch is also specified, mutation info is taken from --scratch-mute-freq-dir, i.e. mutations are *not* from scratch)'}})
subargs['simulate'].append({'name' : '--mutate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for shm-level (mutation) parameters, and instead make up some plausible stuff from scratch'}})
subargs['simulate'].append({'name' : '--flat-mute-freq', 'kwargs' : {'type' : float, 'help' : 'constant mutation frequency, across all positions for all regions and inserts, which is enforced if --mutate-from-scratch is set (but shm rate still varies from sequence to sequence).'}})
subargs['simulate'].append({'name' : '--default-scratch-mute-freq', 'kwargs' : {'type' : float, 'default' : 0.05, 'help' : 'used if --mutate-from-scratch is set, but --flat-mute-freq is not.'}})
subargs['simulate'].append({'name' : '--same-mute-freq-for-all-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'Enforce the same mutation frequency on all sequences (equal to --flat-mute-freq if it\'s set, otherwise the mean of those in the parameter dir).'}})
subargs['simulate'].append({'name' : '--scratch-mute-freq-dir', 'kwargs' : {'default' : partis_dir + '/data/recombinator/scratch-parameters', 'help' : 'synthetic/partial parameter directory with only shm-level (mutation) information, which allows to specify --rearrange-from-scratch without also setting --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Choose a subset of the available genes to represent this sample\'s germline.'}})
subargs['simulate'].append({'name' : '--n-genes-per-region', 'kwargs' : {'default' : glutils.default_n_genes_per_region, 'help' : 'colon-separated list specifying the number of genes (not alleles -- i.e. the *total* number of alleles is this times the number of alleles per gene) for each region (for use with --generate-germline-set)'}})
subargs['simulate'].append({'name' : '--n-sim-alleles-per-gene', 'kwargs' : {'default' : glutils.default_n_alleles_per_gene, 'help' : 'colon-separated list of mean alleles per gene for each region (for use with --generate-germline-set).'}})
subargs['simulate'].append({'name' : '--min-sim-allele-prevalence-freq', 'kwargs' : {'default' : glutils.default_min_allele_prevalence_freq,'type' : float, 'help' : 'minimum frequency at which alleles are allowed to occur, e.g. if it\'s 0.01 then each pair of V alleles will have a prevalence ratio between 0.01 and 1'}})
subargs['simulate'].append({'name' : '--allele-prevalence-fname', 'kwargs' : {'help' : 'abandon help all ye who enter here'}})
subargs['simulate'].append({'name' : '--root-mrca-weibull-parameter', 'kwargs' : {'type' : float, 'help' : 'if set, uses TreeSimGM (instead of TreeSim), and value passed as the parameter (e.g. 0.1: long root-mrca distance, lots of shared mutation; 5: short, little) NOTE requires installation of TreeSimGM'}})

subargs['simulate'].append({'name' : '--subsimproc', 'kwargs' : {'action' : 'store_true', 'help' : 'set to true if this process has subsidiary simulation processes handled by Popen'}})
subargs['simulate'].append({'name' : '--im-a-subproc', 'kwargs' : {'action' : 'store_true', 'help' : 'set to true if this is a sub-process handled by --subsimproc'}})

def get_arg_names(actions):  # return set of all arg names (in the form they appear in args.__dict__) for the specified actions
    if actions == 'all':
        actions = subconfig.keys()
    return set([actionconf['name'][2:].replace('-', '_') for action in actions for actionconf in subargs[action]])

subparsermap = {}
for name, vals in subconfig.items():
    subparsermap[name] = subparsers.add_parser(name, parents=[parent_parser], help=vals['help'], formatter_class=MultiplyInheritedFormatter)
    subparsermap[name].set_defaults(func=vals['func'])
    for argconf in subargs[name]:
        subparsermap[name].add_argument(argconf['name'], **argconf['kwargs'])

# ----------------------------------------------------------------------------------------
args = parser.parse_args()

# add OR of all arguments to all subparsers to <args>, as None (to avoid having to rewrite a *##!(%ton of other code)
for missing_arg in get_arg_names('all') - set(args.__dict__):
    args.__dict__[missing_arg] = None

processargs.process(args)
random.seed(args.seed)
numpy.random.seed(args.seed)
start = time.time()
args.func(args)
print '      total time: %.1f' % (time.time()-start)
