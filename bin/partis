#!/usr/bin/env python
import argparse
import copy
import time
import random
import sys
import csv
import numpy
from multiprocessing import Process, active_children
from subprocess import check_call, check_output, CalledProcessError
import os
partis_dir = os.path.dirname(os.path.realpath(__file__)).replace('/bin', '')
if not os.path.exists(partis_dir):
    print 'WARNING current script dir %s doesn\'t exist, so python path may not be correctly set' % partis_dir
sys.path.insert(1, partis_dir + '/python')

import utils
import glutils
from partitiondriver import PartitionDriver

# ----------------------------------------------------------------------------------------
def default_parameter_dir(args):
    if args.infname is not None:
        label = args.infname[ : args.infname.rfind('.')]  # probably ok not to use the full path here
        label = label.replace('/', '_')
    else:
        label = 'no-need-for-a-name'  # maybe actually don't have a need for one, if there's no input file
    pdir = '_output/' + label
    return pdir

# ----------------------------------------------------------------------------------------
def set_default_parameter_dir(args):
    if args.parameter_dir is None and not args.simulate_partially_from_scratch:
        args.parameter_dir = default_parameter_dir(args)
        print '  using default parameter dir \'%s\'' % args.parameter_dir

# ----------------------------------------------------------------------------------------
def run_simulation(args):
    # I honestly have no recollection of why I started putting the list getting in the sub functions, maybe I should combine it
    args.n_genes_per_region = utils.get_arg_list(args.n_genes_per_region, intify=True)
    args.n_genes_per_region = {r : args.n_genes_per_region[utils.regions.index(r)] for r in utils.regions}  # convert to dict for easier access
    args.n_alleles_per_gene = utils.get_arg_list(args.n_alleles_per_gene)
    assert len(args.n_alleles_per_gene) == len(utils.regions)
    args.n_alleles_per_gene = {utils.regions[i] : [int(n) for n in args.n_alleles_per_gene[i].split(',')] for i in range(len(utils.regions))}

    from recombinator import Recombinator

    if args.slurm and args.n_procs > 1 and not args.subsimproc:
        print '  %s setting subsimproc' % utils.color('red', 'warning')
        args.subsimproc = True
    if args.n_trees is None:
        args.n_trees = max(1, int(float(args.n_sim_events) / args.n_procs))
    if args.outfname is None:
        raise Exception('have to specify --outfname for simulation')
    if args.n_max_queries != -1:
        print '  note: --n-max-queries is not used when simulating (use --n-sim-events to set the simulated number of rearrangemt events)'
    if args.parameter_dir is None and not args.simulate_partially_from_scratch:
        raise Exception('either --parameter-dir must be specified, or --simulate-partially-from-scratch must be set')
    if args.simulate_partially_from_scratch and args.parameter_dir is not None:
        raise Exception('you can\'t specify --parameter-dir if you also set --simulate-partially-from-scratch')
    if args.parameter_dir is not None and args.initial_germline_dir is not None:
        raise Exception('you can\'t specify both --parameter-dir and --inital-germline-dir')
    default_prevalence_fname = args.workdir + '/allele-prevalence.csv'
    if args.generate_germline_set and args.allele_prevalence_fname is None:
        args.allele_prevalence_fname = default_prevalence_fname

    if args.initial_germline_dir is None:  # if the gl info dir wasn't explicitly set on the command line, we have to decide what to use
        if args.parameter_dir is not None:  # if --parameter-dir was explicitly set, assume there's gl info in this --parameter-dir
            input_gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
        else:  # otherwise use the default
            input_gldir = args.default_initial_germline_dir
    else:
        input_gldir = args.initial_germline_dir
    glfo = glutils.read_glfo(input_gldir, args.chain, only_genes=args.only_genes)
    if not args.im_a_subproc and args.simulate_partially_from_scratch and args.generate_germline_set:
        glutils.remove_v_genes_with_bad_cysteines(glfo)
        glutils.generate_germline_set(glfo, args.n_genes_per_region, args.n_alleles_per_gene, args.min_allele_prevalence_freq, args.allele_prevalence_fname)  # NOTE removes unwanted genes from <glfo>
        glutils.write_glfo(args.outfname.replace('.csv', '-glfo'), glfo)
    if args.subsimproc:
        working_gldir = args.workdir + '/' + glutils.glfo_dir
        glutils.write_glfo(working_gldir, glfo)

    # ----------------------------------------------------------------------------------------
    def get_subproc_cmd_str(n_events, iproc, workdir, outfname):
        clist = copy.deepcopy(sys.argv)
        utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
        utils.remove_from_arglist(clist, '--subsimproc')
        clist.append('--im-a-subproc')
        utils.replace_in_arglist(clist, '--seed', str(args.seed + iproc))
        utils.replace_in_arglist(clist, '--workdir', workdir)
        utils.replace_in_arglist(clist, '--outfname', outfname)
        utils.replace_in_arglist(clist, '--n-sim-events', str(n_events))
        utils.replace_in_arglist(clist, '--initial-germline-dir', working_gldir)
        if args.allele_prevalence_fname is not None:
            utils.replace_in_arglist(clist, '--allele-prevalence-fname', args.allele_prevalence_fname)
        if args.slurm:
            clist.insert(0, 'srun')
        return ' '.join(clist)

    # ----------------------------------------------------------------------------------------
    def make_events(n_events, iproc, workdir, outfname, random_ints):
        reco = Recombinator(args, glfo, seed=args.seed+iproc, workdir=workdir, outfname=outfname)
        for ievt in range(n_events):
            reco.combine(random_ints[ievt])

    def get_workdir(iproc):
        return args.workdir + '/sub-' + str(iproc)
    def get_outfname(iproc):
        return get_workdir(iproc) + '/' + os.path.basename(args.outfname)

    if not args.im_a_subproc:
        print 'simulating'

    set_default_parameter_dir(args)

    n_per_proc = int(float(args.n_sim_events) / args.n_procs)

    # generate all the random seeds NOTE these aren't used if <args.subsimproc> is set, i.e. results will be different with and without <args.subsimproc> set.
    all_random_ints = []
    for iproc in range(args.n_procs):  # have to do it all at once, 'cause each of the subprocesses is going to reset its seed and god knows what happens to our seed at that point
        all_random_ints.append([random.randint(0, numpy.iinfo(numpy.int32).max) for i in range(n_per_proc)])

    if args.n_procs == 1:  # multiprocessing is kind of messy
        make_events(n_per_proc, 0, args.workdir, args.outfname, all_random_ints[0])
    else:  # start the processes and wait for 'em to finish
        cmdfos = [{'cmd_str' : get_subproc_cmd_str(n_per_proc, iproc, get_workdir(iproc), get_outfname(iproc)) if args.subsimproc else None,
                   'workdir' : get_workdir(iproc),
                   'logdir' : args.workdir + '/log-' + str(iproc),  # have to be different than <workdirs> since ./bin/partis barfs if its workdir already exists (as it should)
                   'outfname' : get_outfname(iproc)}
                  for iproc in range(args.n_procs)]
        if args.subsimproc:
            utils.run_cmds(cmdfos, debug='print')
        else:
            for iproc in range(args.n_procs):
                proc = Process(target=make_events, args=(n_per_proc, iproc, cmdfos[iproc]['workdir'], cmdfos[iproc]['outfname'], all_random_ints[iproc]))
                proc.start()
            while len(active_children()) > 0:
                sys.stdout.flush()
                time.sleep(1)

        # check and merge output
        n_total_events = 0
        for iproc in range(args.n_procs):
            fname = cmdfos[iproc]['outfname']
            if not os.path.exists(fname):
                raise Exception('output simulation file %s d.n.e.' % fname)
            n_events = int(check_output('grep -v unique_id ' + fname + ' | awk -F, \'{print $2}\' | uniq | wc -l', shell=True).split()[0])  # if *adjacent* events have the same rearrangement parameters (and hence reco id), they'll show up as the same event. It has happened!
            n_total_events += n_events
            if n_events < n_per_proc - 3:  # give ourselves a little breathing room for adjacent events that have the same rearrangement parameters (it's only happened once, ever, so maybe 3 is enough?)
                raise Exception('only found %d events (expected %d) in output file %s' % (n_events, n_per_proc, fname))
        utils.merge_csvs(args.outfname, [cmdfos[i]['outfname'] for i in range(args.n_procs)])
        print '   read %d events from %d files' % (n_total_events, args.n_procs)

    if not args.im_a_subproc and args.allele_prevalence_fname is not None:  # check final prevalence freqs
        glutils.check_allele_prevalence_freqs(args.outfname, glfo, args.allele_prevalence_fname, only_region='v')
        if args.allele_prevalence_fname == default_prevalence_fname:
            os.remove(default_prevalence_fname)

    if args.subsimproc:
        glutils.remove_glfo_files(working_gldir, args.chain)
        for iproc in range(args.n_procs):
            os.rmdir(cmdfos[iproc]['logdir'])

    if not args.im_a_subproc:
        try:
            os.rmdir(args.workdir)
        except OSError:
            raise Exception('workdir (%s) not empty: %s' % (args.workdir, ' '.join(os.listdir(args.workdir))))  # hm... you get weird recursive exceptions if you get here. Oh, well, it still works

# ----------------------------------------------------------------------------------------
def run_partitiondriver(args):
    args.queries = utils.get_arg_list(args.queries)
    args.reco_ids = utils.get_arg_list(args.reco_ids)
    args.n_max_per_region = utils.get_arg_list(args.n_max_per_region, intify=True)
    args.initial_match_mismatch = utils.get_arg_list(args.initial_match_mismatch, intify=True)
    args.annotation_clustering_thresholds = utils.get_arg_list(args.annotation_clustering_thresholds, floatify=True)
    args.naive_hamming_bounds = utils.get_arg_list(args.naive_hamming_bounds, floatify=True)
    if args.seed_unique_id is not None:
        args.seed_unique_id = args.seed_unique_id.strip()  # protect against the space you have to put in front of it if it's got an initial minus sign
    if args.sw_debug is None:  # if not explicitly set, set equal to regular debug
        args.sw_debug = args.debug
    if len(args.n_max_per_region) != 3:
        raise Exception('n-max-per-region should be of the form \'x:y:z\', but I got ' + str(args.n_max_per_region))
    if len(args.initial_match_mismatch) != 2:
        raise Exception('--initial-match-mismatch should be of the form \'match:mismatch\', but I got ' + str(args.n_max_per_region))
    if args.infname is None:
        raise Exception('--infname is required for the \'%s\' action' % args.action)

    set_default_parameter_dir(args)

    if args.action == 'cache-parameters':
        parter = PartitionDriver(args, 'cache-parameters', args.default_initial_germline_dir if args.initial_germline_dir is None else args.initial_germline_dir)  # if we're caching parameters, read initial gl info from <args.initial_germline_dir> (then the final glfo gets written to the parameter dir)
        parter.cache_parameters()
        parter.clean()
        return

    if not os.path.exists(args.parameter_dir):
        print '  parameter dir \'%s\' does not exist, so first caching a new set of parameters' % args.parameter_dir
        newargv = copy.deepcopy(sys.argv)
        cache_parameter_arg_names = [argconf['name'] for argconf in subargs['cache-parameters']]
        for argconf in subargs[args.action]:  # remove args that make sense for <args.action>, but not for parameter caching
            if argconf['name'] in newargv and argconf not in cache_parameter_arg_names:
                index = newargv.index(argconf['name'])
                newargv.remove(argconf['name'])
                if 'action' not in argconf['kwargs'] or argconf['kwargs']['action'] != 'store_true':  # also remove the argument's argument
                    newargv.pop(index)
        newargv = newargv[:1] + ['cache-parameters', ] + newargv[2:]
        check_call(newargv)

    parter = PartitionDriver(args, args.action, args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir)
    if 'run-' in args.action:
        parter.run_algorithm(args.action.replace('run-', ''))
    elif args.action == 'partition':
        parter.partition()
    else:
        raise Exception('bad action ' + args.action)
    parter.clean()

# ----------------------------------------------------------------------------------------
def view_existing_output(args):
    if args.outfname is None:
        raise Exception('--outfname is required for action \'%s\'' % args.action)
    if not os.path.exists(args.outfname):
        raise Exception('--outfname \'%s\' does not exist' % args.outfname)

    set_default_parameter_dir(args)  # all the calls to this function could probably be combined

    if args.initial_germline_dir is None:
        args.initial_germline_dir = args.default_initial_germline_dir

    parter = PartitionDriver(args, args.action, args.initial_germline_dir)
    if args.action == 'view-annotations':
        parter.view_existing_annotations()
    elif args.action == 'view-partitions':
        parter.view_existing_partitions()
    else:
        assert False
    parter.clean()

# ----------------------------------------------------------------------------------------
class MultiplyInheritedFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass
formatter_class = MultiplyInheritedFormatter
parser = argparse.ArgumentParser(formatter_class=MultiplyInheritedFormatter)
subparsers = parser.add_subparsers(dest='action')

parent_parser = argparse.ArgumentParser(add_help=False)
parent_parser.add_argument('--debug', type=int, default=0, choices=[0, 1, 2], help='Debug verbosity level.')
parent_parser.add_argument('--sw-debug', type=int, choices=[0, 1, 2], help='Debug level for Smith-Waterman.')
parent_parser.add_argument('--is-data', action='store_true', help='deprecated! use --is-simu for simulation')
parent_parser.add_argument('--is-simu', action='store_true', help='Set if running on simulated sequences')
parent_parser.add_argument('--skip-unproductive', action='store_true', help='Skip sequences which Smith-Waterman determines to be unproductive (they have stop codons, are out of frame, etc.)')
parent_parser.add_argument('--seed', type=int, default=int(time.time()), help='Random seed for use (mostly) by recombinator (to allow reproducibility)')
parent_parser.add_argument('--no-indels', action='store_true', help='Skip smith-waterman matches that include indels. Note that this just *skips* them, you probably also want to increase the gap-open penalty to prevent vdjalign from finding them in the first place.')
parent_parser.add_argument('--only-csv-plots', action='store_true', help='only write csv plots')
parent_parser.add_argument('--seqfile', help='DEPRECATED use --infname instead')
parent_parser.add_argument('--infname', help='input sequence file in .fa, .fq or .csv (if .csv, specify id string and sequence headers with --name-column and --seq-column)')
parent_parser.add_argument('--name-column', default='unique_id', help='csv column name for sequence ids')
parent_parser.add_argument('--seq-column', default='seq', help='csv column name for nucleotide sequences')
parent_parser.add_argument('--parameter-dir', help='Directory to/from which to write/read sample-specific parameters. If not specified, a default location is used (and printed to std out). If it does not exist, we infer parameters before proceeding to the desired action.')
parent_parser.add_argument('--parameter-type', default='hmm', choices=('sw', 'hmm'), help='Use parameters from Smith-Waterman (sw) or the HMM (hmm) subdirectories for inference/simulation? (you should almost certainly use the hmm ones, but sw is occasionally useful for debugging)')
parent_parser.add_argument('--default-initial-germline-dir', default=partis_dir + '/data/germlines', help='for internal use only')
parent_parser.add_argument('--simulation-germline-dir', help='Germline directory that was used for simulation (not usually needed, in fact maybe only for performance evaluation)')
parent_parser.add_argument('--outfname', help='Output file name.')
parent_parser.add_argument('--sw-cachefname', help='Smith-Waterman cache file name. Default is set using a hash of all the input sequence ids (in partitiondriver, since we have to read the input file first).')
parent_parser.add_argument('--presto-output', action='store_true', help='write output file in presto format')
parent_parser.add_argument('--aligned-germline-fname', help='fasta file with alignments for each V gene')
parent_parser.add_argument('--plotdir', help='Base directory to which to write plots (no plots are written if this isn\'t set)')
parent_parser.add_argument('--ig-sw-binary', default=partis_dir + '/packages/ig-sw/src/ig_align/ig-sw', help='Path to ig-sw executable.')
parent_parser.add_argument('--workdir', help='Temporary working directory')
parent_parser.add_argument('--persistent-cachefname', help='Name of file which will be used as an initial cache file (if it exists), and to which all cached info will be written out before exiting.')
parent_parser.add_argument('--abbreviate', action='store_true', help='Abbreviate/translate sequence ids to improve readability of partition debug output. Uses a, b, c, ..., aa, ab, ...')
parent_parser.add_argument('--dont-remove-framework-insertions', action='store_true', help='')
parent_parser.add_argument('--n-procs', default='1', help='Number of processes over which to parallelize (Can be colon-separated list: first number is procs for hmm, second (should be smaller) is procs for smith-waterman)')
parent_parser.add_argument('--n-max-procs', default=100, help='Never allow more processes than this (default %(default)d)')
parent_parser.add_argument('--n-max-to-calc-per-process', default=200, help='if a bcrham process calc\'d more than this many fwd + vtb values, don\'t decrease the number of processes in the next step (default %(default)d)')
parent_parser.add_argument('--slurm', action='store_true', help='Run multiple processes with slurm, otherwise just runs them on local machine. NOTE make sure to set <workdir> to something visible on all batch nodes.')
parent_parser.add_argument('--queries', help='Colon-separated list of query names to which we restrict ourselves')
parent_parser.add_argument('--reco-ids', help='Colon-separated list of rearrangement-event IDs to which we restrict ourselves')  # or recombination events
parent_parser.add_argument('--n-max-queries', type=int, default=-1, help='Maximum number of query sequences on which to run (except for simulator, where it\'s the number of rearrangement events)')
parent_parser.add_argument('--only-genes', help='Colon-separated list of genes to which to restrict the analysis. If any regions (V/D/J) are not represented among these genes, these regions are left unrestricted.')
parent_parser.add_argument('--n-max-per-region', default='3:5:2', help='Number of best smith-waterman matches (per region, in the format v:d:j) to pass on to the hmm')
parent_parser.add_argument('--default-v-fuzz', type=int, default=2, help='Size of the k space region over which to sum in the v direction, for each single gene match (we take the OR of the bounds over all matches, though, which generally makes the space much bigger)')
parent_parser.add_argument('--default-d-fuzz', type=int, default=2, help='Size of the k space region over which to sum in the d direction, for each single gene match (we take the OR of the bounds over all matches, though, which generally makes the space much bigger)')
parent_parser.add_argument('--gap-open-penalty', type=int, default=30, help='Penalty for indel creation in Smith-Waterman step.')
parent_parser.add_argument('--initial-match-mismatch', default='5:1', help='Initial match:mismatch scores for smith-waterman (we run several iterations, increasing mismatch each time).')
parent_parser.add_argument('--max-logprob-drop', type=float, default=5., help='stop glomerating when the total logprob has dropped by this much')
parent_parser.add_argument('--n-sets', type=int, default=1, help='Separate sequences into sets of size <n> before passing to hmm (i.e. \"k-hmm\").')
parent_parser.add_argument('--all-combinations', action='store_true', help='Run algorithm on *all* possible combinations of the input queries of length <n-sets> (otherwise, if <n-sets> is set, we run on sequential sets of <n-sets> in the input file).')
parent_parser.add_argument('--plot-performance', action='store_true', help='Write out plots comparing true and inferred distributions')
parent_parser.add_argument('--dont-rescale-emissions', action='store_true', help='Don\'t scale each hmm\'s emission probabilities to account for the branch length of each individual sequence.')
parent_parser.add_argument('--print-git-commit', action='store_true', help='print sys.argv, git commit hash, and tag info')
parent_parser.add_argument('--min-observations-to-write', type=int, default=20, help='When writing hmm model files, if we see a gene version fewer times than this, we average over other alleles, or other primary versions, etc. (see hmmwriter). NOTE default is manipulated in partitiondriver.py')
parent_parser.add_argument('--chain', default='h', choices=('h', 'k', 'l'), help='light chain is under development! only heavy chain works at the moment. But in the future: Are these heavy chain, or kappa or lambda light chain?')
parent_parser.add_argument('--species', default='human', choices=('human', 'mouse'), help='Which species?')
parent_parser.add_argument('--only-smith-waterman', action='store_true', help='Exit after finishing smith-waterman.')
# parent_parser.add_argument('--use_mean_at_boundaries', action='store_true', help='see note in hmmwriter')

# am I not using translation when getting the log prob for the final partition?'
# add flag to disable cdr3 preclustering UPDATE actually I think it makes more sense to "allow" shm i indels within cdr3 after doing all the rest of the partitioning'
# make padding specific to each cdr3 subclass UPDATE maybe not worthwhile? doesn't seem to be an exorbitant amount of padding any more'
# don't warn about queries missing from partition if they've been removed because of --seed-unique-id
# fix nonetype argument bullshit when auto-caching-parameters (maybe best to call a ./bin/partis subprocess?)
# is best-minus-x stuff still working?
# need to do allele removal for j (and maybe d?)
# seed with naive sequences
# don't write unseeded uids to output file when seed partitioning
# n_max_per_region needs to be enforced in waterer when getting OR kbounds (should probably actually switch to discarding the extra matches immediately in waterer)
# remove duplicates before writing partition output (?)

# test new gldir/cache-parameters stuff in bin/partis (also make sure it actually removed the warnings)

# large cluster annotations seem to be way too willing to use ridiculously long insertions (e.g. in /fh/fast/matsen_e/processed-data/partis/kate-qrs-2016-09-09/seeds/QB850.402-Vh/Hs-LN1-5RACE-IgG-cluster-annotations.csv)

# stop calling Clear() on dphandler (see notes on the Run() calls)

# I suspect I should sometimes allow translation to allow more than seven/five/whatever simultaneous sequences

# see how much all the translation bullshit actually helps

# get ham scons test working again

# (see if you can) fix thing at line 213 in dphandler.cc

# bryan's super-short-read sample
#  - better insertion mutation rate?
#  - print cdr3 mutation rate (and maybe also make plots without dividing by length?)

# memory usage:
#  - run-viterbi also shows monotonically-increasing behavior (i.e. it's probably the cached trellisi in dphandler, not the cached stuff in glomerator)
#  - at least with run-viterbi, it successfully deallocates most everything between vtb calculations (this may or may not have been true when I was reusing dphandlers)
#  - only seems to use a hundred MB or so on single sequences (it goes up, but it also goes down) -- multi-hmm is the culprit
#  - because it's (at least now) deallocating between queries, I think this isn't a fragmentation problem -- I think the dphandler's cachefo_ is just huge when there's many sequences
#  - i think i could switch to running all the ksets of each gene at once, but this would involve a lot of careful rewriting in dphandler
#  - first see if you can cache fewer


# print_reco_event:
#    - don't print logprob by default
#    - uid on each line
#    - maybe simulation line shouldn't be associated with a particular inferred line (see below)
#    - print \'seed\' in cluster annotations
#    - fix multiple-indel printing'
#    - just print the length for large fv/jf insertions

# testing:
#  - maybe use larger sample sizes
#  - at least make sure the seed cluster is big
#  - move testing off --only-genes

# allele crap:
#     all alleles (including new ones) should have roughly the same mut freq distribution (especially at low mutation)'
#     collapse at least the largest clones before inferring new alleles'
#     finish switch from fitfo to self.fitfos'
#     maybe:
#       y-icpt (i.e. allele prevalence) could also go into the decision about whether to remove the template gene or not'
#       add requirement that all positions in multiple-snp alleles are correlated (?)'
#       do all the plots separately for each potential original snp base (i.e. separate plot for A, C, G, T)?'
#       add requirement for mulitple j genes for new alleles (?)'
#       increase n_max_mutations_per_segment for highly-mutated repertoires (?)'

subconfig = {
    'version'          : {'func' : int, 'help' : 'print version information and exit'},
    'cache-parameters' : {'func' : run_partitiondriver, 'help' : 'Cache parameter values and write hmm model files.'},
    'run-viterbi'      : {'func' : run_partitiondriver, 'help' : 'Annotate sequences in input file, i.e. run the viterbi algorithm, using pre-existing parameter directory.'},
    'partition'        : {'func' : run_partitiondriver, 'help' : 'Partition sequences in input file into clonally-related families using pre-existing parameter directory.'},
    'simulate'         : {'func' : run_simulation,      'help' : 'Generate simulated sequences based on information in pre-existing parameter directory.'},
    'run-forward'      : {'func' : run_partitiondriver, 'help' : 'Run the forward algorithm on sequences in input file, using pre-existing parameter directory. Not particularly useful except for debugging.'},
    'view-annotations' : {'func' : view_existing_output,'help' : 'Print (to std out) the annotations from an existing annotation output csv.'},
    'view-partitions'  : {'func' : view_existing_output,'help' : 'Print (to std out) the partitions from an existing partition output csv.'}
}

subargs = {subname : [] for subname in subconfig}

subargs['cache-parameters'].append({'name' : '--dont-remove-unlikely-alleles', 'kwargs' : {'action' : 'store_true', 'help' : 'assign each sequence to its closest germline match, even if this would imply an unrealistically large number of alleles per gene in this sample'}})
subargs['cache-parameters'].append({'name' : '--find-new-alleles', 'kwargs' : {'action' : 'store_true', 'help' : 'look for alleles which do not appear in the default germline set'}})
subargs['cache-parameters'].append({'name' : '--always-find-new-alleles', 'kwargs' : {'action' : 'store_true', 'help' : 'By default we only look for new alleles if a repertoire\'s mutation rate is amenable to reasonable new-allele sensitivity (i.e. if it\'s not crazy high). This overrides that.'}})
subargs['cache-parameters'].append({'name' : '--new-allele-fname', 'kwargs' : {'help' : 'fasta fname to which to write any new alleles (they are also written, together with existing alleles, to --parameter-dir)'}})
subargs['cache-parameters'].append({'name' : '--debug-allele-finding', 'kwargs' : {'action' : 'store_true', 'help' : 'print lots of debug info on new-allele fits'}})
subargs['cache-parameters'].append({'name' : '--n-max-allele-finding-iterations', 'kwargs' : {'type' : int, 'default' : 1, 'help' : 'maximum number of times to look for new alleles'}})
subargs['cache-parameters'].append({'name' : '--n-max-snps', 'kwargs' : {'type' : int, 'default' : 8, 'help' : 'when new-allele finding, look for new alleles separated from existing alleles by up to this many SNPs'}})
subargs['cache-parameters'].append({'name' : '--n-max-mutations-per-segment', 'kwargs' : {'type' : int, 'default' : 30, 'help' : 'when new-allele finding, exclude sequences which have more than this many mutations in the V segment'}})
subargs['cache-parameters'].append({'name' : '--n-alleles-per-gene', 'kwargs' : {'type' : int, 'default' : 2, 'help' : 'number of alleles to assume per gene when removing spurious alleles during parameter caching'}})
subargs['cache-parameters'].append({'name' : '--min-allele-prevalence-fraction', 'kwargs' : {'type' : float, 'default' : 0.0005, 'help' : 'Remove any alleles that represent less than this fraction of the repertoire.'}})
subargs['cache-parameters'].append({'name' : '--initial-germline-dir', 'kwargs' : {'help' : 'Directory with fastas from which to read germline set. Only used when caching parameters, during which its contents is copied into --parameter-dir, perhaps (i.e. if specified) with modification. NOTE default is set below, because control flow is too complicated for argparse'}})

subargs['partition'].append({'name' : '--naive-hamming', 'kwargs' : {'action' : 'store_true', 'help' : 'agglomerate purely with naive hamming distance, i.e. set the low and high preclustering bounds to the same value'}})
subargs['partition'].append({'name' : '--naive-vsearch', 'kwargs' : {'action' : 'store_true', 'help' : 'Very fast clustering: infer naive (unmutated ancestor) for each input sequence, then toss it all into vsearch. But, of course, not as accurate as the slower methods.'}})
subargs['partition'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'Only look for sequences that are clonally related to this unique id. Much much much faster than partitioning the entire dataset.'}})
subargs['partition'].append({'name' : '--seed-seq', 'kwargs' : {'help' : 'same as --seed-unique-id, but specifies a sequence that doesn\'t have to be in the original file'}})
subargs['partition'].append({'name' : '--annotation-clustering', 'kwargs' : {'help' : 'Perform annotation-based clustering: group together sequences with the same V and J, same CDR3 length, and 90%% cdr identity. Very, very inaccurate.'}})
subargs['partition'].append({'name' : '--annotation-clustering-thresholds', 'kwargs' : {'default' : '0.9', 'help' : 'colon-separated list of thresholds for annotation-based (e.g. vollmers) clustering'}})
subargs['partition'].append({'name' : '--print-cluster-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'Print annotation for each final cluster. If --outfname is also specified, then the cluster annotations are written to <args.outfname.replace(\'.csv\', \'-cluster-annotations.csv\')>'}})
subargs['partition'].append({'name' : '--naive-hamming-bounds', 'kwargs' : {'help' : 'Clustering bounds (lo:hi colon-separated pair) on naive sequence hamming distance. If not specified, the bounds are set based on the per-dataset mutation levels. For most purposes should be left at the defaults.'}})
subargs['partition'].append({'name' : '--logprob-ratio-threshold', 'kwargs' : {'type' : float, 'default' : 18., 'help' : 'reaches a min value of <this> minus five for large clusters.'}})
subargs['partition'].append({'name' : '--synthetic-distance-based-partition', 'kwargs' : {'action' : 'store_true', 'help' : 'Use simulation truth info to create a synthetic distance-based partition (for validation).'}})
subargs['partition'].append({'name' : '--cache-naive-hfracs', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to naive sequences and log probabilities, also cache naive hamming fractions between cluster pairs. Only really useful for plotting or testing.'}})
subargs['partition'].append({'name' : '--n-precache-procs', 'kwargs' : {'type' : int, 'help' : 'Number of processes to use when precaching naive sequences. Default is set based on some heuristics, and should typically only be overridden for testing.'}})
subargs['partition'].append({'name' : '--biggest-naive-seq-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 7, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--biggest-logprob-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 7, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--n-partitions-to-write', 'kwargs' : {'type' : int, 'default' : 5, 'help' : 'Number of partitions (surrounding the best partition) to write to output file.'}})
subargs['partition'].append({'name' : '--naive-swarm', 'kwargs' : {'action' : 'store_true', 'help' : 'Use swarm instead of vsearch, which the developer recommends. Didn\'t seem to help much, and needs more work to optimize threshold, so DO NOT USE.'}})

subargs['simulate'].append({'name' : '--mutation-multiplier', 'kwargs' : {'type' : float, 'help' : 'Multiply observed branch lengths by some factor when simulating, e.g. if in data it was 0.05, but you want closer to ten percent in your simulation, set this to 2'}})
subargs['simulate'].append({'name' : '--mimic-data-read-length', 'kwargs' : {'action' : 'store_true', 'help' : 'trim V 5\' and D 3\' to mimic read lengths seen in data'}})
subargs['simulate'].append({'name' : '--n-sim-events', 'kwargs' : {'type' : int, 'default' : 1, 'help' : 'Number of rearrangement events to simulate'}})
subargs['simulate'].append({'name' : '--n-trees', 'kwargs' : {'type' : int, 'help' : 'Number of phylogenetic trees from which to choose during simulation (we pre-generate this many trees before starting a simulation run, then for each rearrangement event choose one at random -- so this should be at least of order the number of simulated events, so your clonal families don\'t all have the same tree).'}})
subargs['simulate'].append({'name' : '--n-leaves', 'kwargs' : {'type' : float, 'default' : 5., 'help' : 'Parameter describing the number of leaves per tree (maybe the mean, maybe not -- depends on the distribution)'}})
subargs['simulate'].append({'name' : '--constant-number-of-leaves', 'kwargs' : {'action' : 'store_true', 'help' : 'Give all trees the same number of leaves'}})
subargs['simulate'].append({'name' : '--n-leaf-distribution', 'kwargs' : {'default' : 'geometric', 'choices' : ['geometric', 'box', 'zipf'], 'help' : 'distribution from which to draw the number of leaves for each tree'}})
subargs['simulate'].append({'name' : '--indel-frequency', 'kwargs' : {'default' : 0., 'type' : float, 'help' : 'fraction of simulated sequences with indels'}})
# disabled for now, but if you want multiple indels per sequence you can use this (you'd also need to uncomment a line in recombinator):)
#subargs['simulate'].append({'name' : '--mean-n-indels', 'kwargs' : {'default' : 1, 'type' : int, 'help' : 'mean number of indels in each sequence which we\'ve already decided has indels (geometric distribution)'}})
subargs['simulate'].append({'name' : '--mean-indel-length', 'kwargs' : {'default' : 5, 'help' : 'mean length of each indel (geometric distribution)'}})
subargs['simulate'].append({'name' : '--indel-location', 'kwargs' : {'choices' : [None, 'v', 'cdr3'], 'help' : 'where to put the indels'}})
# NOTE command to generate gtr parameter file: [stoat] partis/ > zcat /shared/silo_researcher/Matsen_F/MatsenGrp/data/bcr/output_sw/A/04-A-M_gtr_tr-qi-gi.json.gz | jq .independentParameters | grep -v '[{}]' | sed 's/["\:,]//g' | sed 's/^[ ][ ]*//' | sed 's/ /,/' | sort >data/gtr.txt)
subargs['simulate'].append({'name' : '--gtrfname', 'kwargs' : {'default' : partis_dir + '/data/recombinator/gtr.txt', 'help' : 'File with list of GTR parameters. Fed into bppseqgen along with the chosen tree. Corresponds to an arbitrary dataset at the moment, but eventually will be inferred per-dataset.'}})
subargs['simulate'].append({'name' : '--simulate-partially-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory to describe the repertoire; instead start from scratch. Mutation freqs are harder, so for now we use some stuff in data/recombinator/.'}})
subargs['simulate'].append({'name' : '--scratch-mute-freq-dir', 'kwargs' : {'default' : partis_dir + '/data/recombinator/scratch-parameters', 'help' : 'see --simulate-partially-from-scratch'}})
subargs['simulate'].append({'name' : '--generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Choose a subset of the available genes to represent this sample\'s germline.'}})
subargs['simulate'].append({'name' : '--n-genes-per-region', 'kwargs' : {'default' : '15:5:3', 'help' : 'colon-separated list specifying the number of genes (not alleles -- i.e. the *total* number of alleles is this times the number of alleles per gene) for each region (for use with --generate-germline-set)'}})
subargs['simulate'].append({'name' : '--n-alleles-per-gene', 'kwargs' : {'default' : '1,2,3:1,2:1,2', 'help' : 'colon-separated list of comma-separated lists of the allowed allele multiplicities for each region, e.g. \'2:1,2:1,2\' means that for each gene we will choose two alleles for V, and either 1 or 2 for D and J (for use with --generate-germline-set)'}})
subargs['simulate'].append({'name' : '--min-allele-prevalence-freq', 'kwargs' : {'default' : 0.1, 'type' : float, 'help' : 'minimum frequency at which alleles are allowed to occur, e.g. if it\'s 0.01 then each pair of V alleles will have a prevalence ratio between --min-allele-prevalence-freq and 1..'}})
subargs['simulate'].append({'name' : '--allele-prevalence-fname', 'kwargs' : {'help' : 'abandon help all ye who enter here'}})
subargs['simulate'].append({'name' : '--initial-germline-dir', 'kwargs' : {'help' : 'Directory with fastas from which to read germline set. Only used when caching parameters, during which its contents is copied into --parameter-dir, perhaps (i.e. if specified) with modification. NOTE default is set below, because control flow is too complicated for argparse'}})

subargs['simulate'].append({'name' : '--subsimproc', 'kwargs' : {'action' : 'store_true', 'help' : 'set to true if this process has subsidiary simulation processes handled by Popen'}})
subargs['simulate'].append({'name' : '--im-a-subproc', 'kwargs' : {'action' : 'store_true', 'help' : 'set to true if this is a sub-process handled by --subsimproc'}})

subparsermap = {}
for name, vals in subconfig.items():
    subparsermap[name] = subparsers.add_parser(name, parents=[parent_parser], help=vals['help'], formatter_class=MultiplyInheritedFormatter)
    subparsermap[name].set_defaults(func=vals['func'])
    for argconf in subargs[name]:
        subparsermap[name].add_argument(argconf['name'], **argconf['kwargs'])

# ----------------------------------------------------------------------------------------
if '--action' in sys.argv:
    print '  NOTE we\'ve switched --action to a positional argument, so you can remove \'--action\' from your command line'
    sys.argv.remove('--action')

args = parser.parse_args()

args.partis_dir = partis_dir

# add OR of all arguments to all subparsers to <args>, as None (to avoid having to rewrite a *##!(%ton of other code)
for name in subconfig:
    for argconf in subargs[name]:
        if argconf['name'][:2] != '--':
            raise Exception('expected argument %s to be of form --<stuff>' % argconf['name'])
        argname = argconf['name'][2:].replace('-', '_')
        if argname not in args.__dict__:
            args.__dict__[argname] = None

if args.only_genes == 'TEST':
    args.only_genes = utils.test_only_genes
args.only_genes = utils.get_arg_list(args.only_genes)
args.n_procs = utils.get_arg_list(args.n_procs, intify=True)
args.n_fewer_procs = args.n_procs[0] if len(args.n_procs) == 1 else args.n_procs[1]
args.n_procs = args.n_procs[0]

if args.seqfile is not None and args.infname is None:
    print '  moving contents of deprecated option --seqfile into its replacement --infname'
    args.infname = args.seqfile

if args.only_genes is not None:
    for gene in args.only_genes:  # make sure they're all at least valid ig genes
        utils.split_gene(gene)

# if n_procs < 1 or n_procs > 9999:  # It happened, at least once. You know, probably.
#     raise Exception('bad n_procs %s' % n_procs)
if args.n_procs > args.n_max_procs:
    print 'reducing n procs %d to --n-max-procs %d' % (args.n_procs, args.n_max_procs)
    args.n_procs = args.n_max_procs
if args.n_fewer_procs > args.n_max_procs:
    print 'reducing n procs %d to --n-max-procs %d' % (args.n_fewer_procs, args.n_max_procs)
    args.n_fewer_procs = args.n_max_procs

if args.print_git_commit or args.action == 'version':
    print 'RUN ' + ' '.join(sys.argv)
    tag = check_output(['git', 'tag']).split()[-1]
    print '       tag %s' % tag
    print '    commit %s' % check_output(['git', 'rev-parse', 'HEAD']).strip()
    if args.action == 'version':
        sys.exit(0)

if args.is_data:  # if <is_data> was set on the command line, print a warning and continue
    print '  NOTE --is-data is no longer needed (it\'s the default; add --is-simu if running on simulation)'
    if args.is_simu:
        raise Exception('--is-data and --is-simu both set on the command line')
elif args.is_simu:
    pass  # args.is_data = False
else:  # if neither was given on the command line, set is_data to True
    args.is_data = True

if args.no_indels and args.gap_open_penalty < 1000:
    print 'forcing --gap-open-penalty to 1000 to prevent indels, since --no-indels was specified (you can also adjust this penalty directly)'
    args.gap_open_penalty = 1000

if args.workdir is None:  # set default here so we know whether it was set by hand or not
    if args.slurm and os.path.exists('/fh/fast/matsen_e'):
        args.workdir = '/fh/fast/matsen_e/dralph/_tmp/hmms/' + str(random.randint(0, 999999))
    else:
        args.workdir = '/tmp/' + os.path.basename(os.getenv('HOME')) + '/hmms/' + str(random.randint(0, 999999))
        if args.slurm:
            print '  %s: using slurm with default --workdir (%s) -- if this isn\'t visible to the batch nodes on your system, you\'ll need to change it' % (utils.color('red', 'warning'), args.workdir)
else:
    args.workdir = args.workdir.rstrip('/')
if os.path.exists(args.workdir):
    raise Exception('workdir %s already exists' % args.workdir)

if args.plot_performance:
    if args.plotdir is None:
        raise Exception('can\'t plot performance unless --plotdir is specified')
    if not args.is_simu:
        raise Exception('can\'t plot performance unless --is-simu is set')

if args.parameter_type != 'hmm':
    print '  using non-default parameter type \'%s\'' % args.parameter_type

if args.presto_output and args.aligned_germline_fname is None:
    raise Exception('in order to get presto output, you have to set --aligned-germline-fname (a fasta file with germline alignments for every germline gene)')

if args.parameter_dir is not None:
    args.parameter_dir = args.parameter_dir.rstrip('/')

args.default_initial_germline_dir += '/' + args.species

if args.n_max_snps is not None and args.n_max_mutations_per_segment is not None:
    if args.n_max_snps > args.n_max_mutations_per_segment - 10:
        raise Exception('--n-max-snps should be at least ten less than --n-max-mutations-per-segment, but I got %d and %d' % (args.n_max_snps, args.n_max_mutations_per_segment))

# ----------------------------------------------------------------------------------------
random.seed(args.seed)
numpy.random.seed(args.seed)
start = time.time()
args.func(args)
print '      total time: %.1f' % (time.time()-start)
