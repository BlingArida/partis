#!/usr/bin/env python
from distutils.version import StrictVersion
import argparse
import copy
import time
import random
import sys
import csv
import numpy
import scipy
if StrictVersion(scipy.__version__) < StrictVersion('0.17.0'):
    raise RuntimeError("scipy version 0.17 or later is required (found version %s)." % scipy.__version__)
import colored_traceback.always
import multiprocessing
from subprocess import check_call, check_output, CalledProcessError
import os
partis_dir = os.path.dirname(os.path.realpath(__file__)).replace('/bin', '')
if not os.path.exists(partis_dir):
    print 'WARNING current script dir %s doesn\'t exist, so python path may not be correctly set' % partis_dir
sys.path.insert(1, partis_dir + '/python')
sys.path.insert(1, partis_dir + '/packages/baltic')

import utils
import glutils
from partitiondriver import PartitionDriver
from partitionplotter import PartitionPlotter

# ----------------------------------------------------------------------------------------
def default_parameter_dir(args):
    if args.infname is not None:
        label = args.infname[ : args.infname.rfind('.')]  # probably ok not to use the full path here
        label = label.replace('/', '_')
    else:
        label = 'xxx-dummy-xxx'  # gah, this is a terrible way to do this. But I can't set it to None, and I need some way to communicate that it isn't a valid pdir, so...
    pdir = '_output/' + label
    return pdir

# ----------------------------------------------------------------------------------------
def set_default_parameter_dir(args):
    if args.parameter_dir is None and not args.rearrange_from_scratch and not args.mutate_from_scratch:
        args.parameter_dir = default_parameter_dir(args)

# ----------------------------------------------------------------------------------------
def run_simulation(args):
    from recombinator import Recombinator

    if args.batch_system is not None and args.n_procs > 1 and not args.subsimproc:
        print '  %s setting subsimproc' % utils.color('red', 'warning')
        args.subsimproc = True
    if args.n_trees is None:
        args.n_trees = max(1, int(float(args.n_sim_events) / args.n_procs))
    if args.outfname is None:
        raise Exception('have to specify --outfname for simulation')
    if args.n_max_queries != -1:
        print '  note: --n-max-queries is not used when simulating (use --n-sim-events to set the simulated number of rearrangemt events)'

    if args.parameter_dir is None:
        if not args.rearrange_from_scratch and not args.mutate_from_scratch:
            raise Exception('either --parameter-dir must be specified, or one of the scratch simulation options')
        if args.mutate_from_scratch and not args.rearrange_from_scratch:
            raise Exception('haven\'t yet implemented mutating from scratch without rearranging from scratch')  # and maybe not ever
    else:
        if args.rearrange_from_scratch or args.mutate_from_scratch or args.generate_germline_set:
            raise Exception('you can\'t specify --parameter-dir if you also set either of the scratch options or --generate-germline-set')
        if args.initial_germline_dir is not None:
            raise Exception('you can\'t specify both --parameter-dir and --inital-germline-dir (germline info has to be read from the parameter dir)')

    utils.prep_dir(args.workdir)

    default_prevalence_fname = args.workdir + '/allele-prevalence.csv'
    if args.generate_germline_set and args.allele_prevalence_fname is None:
        args.allele_prevalence_fname = default_prevalence_fname

    if args.initial_germline_dir is None:  # if the gl info dir wasn't explicitly set on the command line, we have to decide what to use
        if args.parameter_dir is not None:  # if --parameter-dir was explicitly set, assume there's gl info in this --parameter-dir
            input_gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
        else:  # otherwise use the default
            input_gldir = args.default_initial_germline_dir
    else:
        input_gldir = args.initial_germline_dir
    glfo = glutils.read_glfo(input_gldir, args.locus, only_genes=args.only_genes)
    if not args.im_a_subproc and args.rearrange_from_scratch and args.generate_germline_set:
        glutils.generate_germline_set(glfo, args.n_genes_per_region, args.n_sim_alleles_per_gene, args.min_sim_allele_prevalence_freq, args.allele_prevalence_fname)  # NOTE removes unwanted genes from <glfo>
        print '  writing generated germline set to %s/' % args.outfname.replace('.csv', '-glfo')
        glutils.write_glfo(args.outfname.replace('.csv', '-glfo'), glfo)
    if args.subsimproc:
        working_gldir = args.workdir + '/' + glutils.glfo_dir
        glutils.write_glfo(working_gldir, glfo)

    # ----------------------------------------------------------------------------------------
    def get_subproc_cmd_str(n_events, iproc, workdir, outfname):
        clist = copy.deepcopy(sys.argv)
        utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
        utils.remove_from_arglist(clist, '--subsimproc')
        clist.append('--im-a-subproc')
        utils.replace_in_arglist(clist, '--seed', str(args.seed + iproc))
        utils.replace_in_arglist(clist, '--workdir', workdir)
        utils.replace_in_arglist(clist, '--outfname', outfname)
        utils.replace_in_arglist(clist, '--n-sim-events', str(n_events))
        utils.replace_in_arglist(clist, '--initial-germline-dir', working_gldir)
        if args.allele_prevalence_fname is not None:
            utils.replace_in_arglist(clist, '--allele-prevalence-fname', args.allele_prevalence_fname)
        return ' '.join(clist)

    # ----------------------------------------------------------------------------------------
    def make_events(n_events, iproc, workdir, outfname, random_ints):
        reco = Recombinator(args, glfo, seed=args.seed+iproc, workdir=workdir, outfname=outfname)
        for ievt in range(n_events):
            reco.combine(random_ints[ievt])
        # reco.print_validation_values()

    def get_workdir(iproc):
        return args.workdir + '/sub-' + str(iproc)
    def get_outfname(iproc):
        return get_workdir(iproc) + '/' + os.path.basename(args.outfname)

    if not args.im_a_subproc:
        print 'simulating'

    set_default_parameter_dir(args)

    n_per_proc = int(float(args.n_sim_events) / args.n_procs)

    # generate all the random seeds NOTE these aren't used if <args.subsimproc> is set, i.e. results will be different with and without <args.subsimproc> set.
    all_random_ints = []
    for iproc in range(args.n_procs):  # have to do it all at once, 'cause each of the subprocesses is going to reset its seed and god knows what happens to our seed at that point
        all_random_ints.append([random.randint(0, numpy.iinfo(numpy.int32).max) for i in range(n_per_proc)])

    if args.n_procs == 1:  # multiprocessing is kind of messy
        make_events(n_per_proc, 0, args.workdir, args.outfname, all_random_ints[0])
    else:  # start the processes and wait for 'em to finish
        cmdfos = [{'cmd_str' : get_subproc_cmd_str(n_per_proc, iproc, get_workdir(iproc), get_outfname(iproc)) if args.subsimproc else None,
                   'workdir' : get_workdir(iproc),
                   'logdir' : args.workdir + '/log-' + str(iproc),  # have to be different than <workdirs> since ./bin/partis barfs if its workdir already exists (as it should)
                   'outfname' : get_outfname(iproc)}
                  for iproc in range(args.n_procs)]
        if args.subsimproc:
            utils.run_cmds(cmdfos, batch_system=args.batch_system, batch_options=args.batch_options, batch_config_fname=args.batch_config_fname, debug='print')
        else:
            for iproc in range(args.n_procs):
                proc = multiprocessing.Process(target=make_events, args=(n_per_proc, iproc, cmdfos[iproc]['workdir'], cmdfos[iproc]['outfname'], all_random_ints[iproc]))
                proc.start()
            while len(multiprocessing.active_children()) > 0:
                sys.stdout.flush()
                time.sleep(1)

        # check and merge output
        n_total_events = 0
        for iproc in range(args.n_procs):
            fname = cmdfos[iproc]['outfname']
            if not os.path.exists(fname):
                raise Exception('output simulation file %s d.n.e.' % fname)
            n_events = int(check_output('grep -v unique_id ' + fname + ' | awk -F, \'{print $2}\' | uniq | wc -l', shell=True).split()[0])  # if *adjacent* events have the same rearrangement parameters (and hence reco id), they'll show up as the same event. It has happened!
            n_total_events += n_events
            if n_events < n_per_proc - 3:  # give ourselves a little breathing room for adjacent events that have the same rearrangement parameters (it's only happened once, ever, so maybe 3 is enough?)
                raise Exception('only found %d events (expected %d) in output file %s' % (n_events, n_per_proc, fname))
        utils.merge_csvs(args.outfname, [cmdfos[i]['outfname'] for i in range(args.n_procs)])
        print '   read %d events from %d files' % (n_total_events, args.n_procs)

    if not args.im_a_subproc and args.allele_prevalence_fname is not None:  # check final prevalence freqs
        glutils.check_allele_prevalence_freqs(args.outfname, glfo, args.allele_prevalence_fname, only_region='v')
        if args.allele_prevalence_fname == default_prevalence_fname:
            os.remove(default_prevalence_fname)

    if args.subsimproc:
        glutils.remove_glfo_files(working_gldir, args.locus)
        for iproc in range(args.n_procs):
            os.rmdir(cmdfos[iproc]['logdir'])

    if not args.im_a_subproc:
        try:
            os.rmdir(args.workdir)
        except OSError:
            raise Exception('workdir (%s) not empty: %s' % (args.workdir, ' '.join(os.listdir(args.workdir))))  # hm... you get weird recursive exceptions if you get here. Oh, well, it still works

# ----------------------------------------------------------------------------------------
def run_partitiondriver(args):
    if len(args.n_max_per_region) != 3:
        raise Exception('n-max-per-region should be of the form \'x:y:z\', but I got ' + str(args.n_max_per_region))
    if len(args.initial_match_mismatch) != 2:
        raise Exception('--initial-match-mismatch should be of the form \'match:mismatch\', but I got ' + str(args.n_max_per_region))
    if args.infname is None:
        raise Exception('--infname is required for the \'%s\' action' % args.action)

    set_default_parameter_dir(args)

    if args.action == 'cache-parameters':
        parter = PartitionDriver(args, 'cache-parameters', args.default_initial_germline_dir if args.initial_germline_dir is None else args.initial_germline_dir)  # if we're caching parameters, read initial gl info from <args.initial_germline_dir> (then the final glfo gets written to the parameter dir)
        parter.cache_parameters()
        parter.clean()
        return

    if not os.path.exists(args.parameter_dir):
        print '  parameter dir \'%s\' does not exist, so caching a new set of parameters before running action \'%s\'' % (args.parameter_dir, args.action)
        newargv = copy.deepcopy(sys.argv)
        for argconf in [ac for ac in subargs[args.action] if ac['name'] in newargv]:  # remove args that only make sense for <args.action>
            index = newargv.index(argconf['name'])
            newargv.remove(argconf['name'])
            if 'action' not in argconf['kwargs'] or argconf['kwargs']['action'] != 'store_true':  # also remove the argument's argument
                newargv.pop(index)
        newargv = newargv[:1] + ['cache-parameters', ] + newargv[2:]
        check_call(newargv)

    if args.action == 'annotate':
        parter = PartitionDriver(args, args.action, args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir)
        parter.annotate()
    elif args.action == 'partition':
        if args.n_partition_subsets is None:
            parter = PartitionDriver(args, args.action, args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir)
            parter.partition()
        else:
            if args.abbreviate:
                raise Exception('not supported (different runs have the same uid for different sequences, which breaks everything)')
            old_outfname = args.outfname
            if args.n_max_queries == -1:
                raise Exception('--n-max-queries must be set in order to use --n-partition-subsets')
            n_per_subset = args.n_max_queries / args.n_partition_subsets
            args.n_max_queries = -1
            outfnames = [args.workdir + '/outfiles/partition-' + str(isub) + '.csv' for isub in range(args.n_partition_subsets)]
            istart = 0
            for isub in range(args.n_partition_subsets):
                istop = istart + n_per_subset
                args.outfname = outfnames[isub]
                args.istartstop = [istart, istop]  # and here I break my rule about not modifying <args>, sigh.
                parter = PartitionDriver(args, args.action, args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir)
                parter.partition()
                istart = istop
            partplotter = PartitionPlotter()
            if args.plotdir is not None:
                partplotter.plot(args.plotdir + '/partitions', infiles=outfnames, only_csv=args.only_csv_plots)
            for fn in outfnames:
                os.remove(fn)
            os.rmdir(os.path.dirname(outfnames[0]))
    else:
        raise Exception('bad action ' + args.action)
    parter.clean()

# ----------------------------------------------------------------------------------------
def view_existing_output(args):
    if args.outfname is None:
        if args.infname is not None:  # can specify either outfname or infname
            args.outfname = args.infname
            args.infname = None  # otherwise seqfileopener tries to read it (you can specify *both* of them on the command line, though, if you want this step to know about the reco info)
        else:
            raise Exception('--outfname is required for action \'%s\'' % args.action)
    if not os.path.exists(args.outfname):
        raise Exception('--outfname \'%s\' does not exist' % args.outfname)

    set_default_parameter_dir(args)  # all the calls to this function could probably be combined

    if args.initial_germline_dir is None:
        if 'xxx-dummy-xxx' in args.parameter_dir:
            args.initial_germline_dir = args.default_initial_germline_dir
        else:
            args.initial_germline_dir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir

    parter = PartitionDriver(args, args.action, args.initial_germline_dir)
    if args.action == 'view-annotations':
        parter.read_existing_annotations(debug=True)
    elif args.action == 'view-partitions':
        parter.read_existing_partitions(debug=True)
    elif args.action == 'plot-partitions':
        parter.plot_existing_partitions()
    elif args.action == 'view-alternative-naive-seqs':
        parter.view_alternative_naive_seqs()
    else:
        assert False
    parter.clean()

# ----------------------------------------------------------------------------------------
class MultiplyInheritedFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass
formatter_class = MultiplyInheritedFormatter
parser = argparse.ArgumentParser(formatter_class=MultiplyInheritedFormatter)
subparsers = parser.add_subparsers(dest='action')

parent_parser = argparse.ArgumentParser(add_help=False)

parent_parser.add_argument('--locus', default='igh', choices=utils.loci.keys(), help='which immunoglobulin or t-cell receptor locus?')
parent_parser.add_argument('--chain', choices=('h', 'k', 'l'), help='DEPRECATED in favor of --locus, exists only for backwards compatibility')
parent_parser.add_argument('--species', default='human', choices=('human', 'mouse'), help='Which species?')
parent_parser.add_argument('--queries', help='Colon-separated list of query names to which to restrict the analysis')
parent_parser.add_argument('--queries-to-include', help='when using --n-random-queries, look for, and include, these additional uids (in contast to --queries, which includes *only* the indicated uids) NOTE *not* compatible with --n-max-queries, i.e. use --n-random-queries instead')
parent_parser.add_argument('--reco-ids', help='Colon-separated list of rearrangement-event IDs to which we restrict ourselves')  # or recombination events
parent_parser.add_argument('--n-max-queries', type=int, default=-1, help='Maximum number of query sequences to read, starting from beginning of input file')
parent_parser.add_argument('--n-random-queries', type=int, help='choose this many queries at random from input file')
parent_parser.add_argument('--istartstop', help='colon-separated start:stop line indices for input sequence file (with python slice conventions, e.g. if set to \'2:4\' will skip the zeroth and first sequences, and then take the following two sequences, and then skip all subsequence sequences). Applied before any other input filters, e.g. --n-max-queries, --queries, --reco-ids, etc.')

parent_parser.add_argument('--debug', type=int, default=0, choices=[0, 1, 2], help='Debug verbosity level.')
parent_parser.add_argument('--sw-debug', type=int, choices=[0, 1, 2], help='Debug level for Smith-Waterman.')
parent_parser.add_argument('--abbreviate', action='store_true', help='Abbreviate/translate sequence ids to improve readability of partition debug output. Uses a, b, c, ..., aa, ab, ...')
parent_parser.add_argument('--print-git-commit', action='store_true', help='print sys.argv, git commit hash, and tag info')

parent_parser.add_argument('--n-procs', default='1', help='Number of processes over which to parallelize (Can be colon-separated list: first number is procs for hmm, second (should be smaller) is procs for smith-waterman)')
parent_parser.add_argument('--n-max-procs', default=100, help='Never allow more processes than this (default %(default)d)')
parent_parser.add_argument('--n-max-to-calc-per-process', default=250, help='if a bcrham process calc\'d more than this many fwd + vtb values (and this is the first time with this number of procs), don\'t decrease the number of processes in the next step (default %(default)d)')
parent_parser.add_argument('--min-hmm-step-time', default=2., help='if a clustering step takes fewer than this many seconds, always reduce n_procs')
parent_parser.add_argument('--batch-system', choices=['slurm', 'sge'], help='batch system with which to attempt paralellization')
parent_parser.add_argument('--batch-options', help='additional options to apply to --batch-system (e.g. --batch-options="--foo bar")')
parent_parser.add_argument('--batch-config-fname', default='/etc/slurm-llnl/slurm.conf', help='system-wide batch system configuration file name')

parent_parser.add_argument('--only-smith-waterman', action='store_true', help='Exit after finishing smith-waterman.')
parent_parser.add_argument('--count-parameters', action='store_true', help='force parameter counting when action is not cache-parameters (presumably so that you can plot them)')
parent_parser.add_argument('--dont-write-parameters', action='store_true', help='don\'t write parameters to disk even if you\'ve counted them (mostly for use in conjunction with --only-smith-waterman, so you can avoid cluttering up your file system)')
parent_parser.add_argument('--write-trimmed-and-padded-seqs-to-sw-cachefname', action='store_true', help='after running sw, trim and pad sequences *before* writing to the sw cache file (rather than after). Note that this will in some cases cause waterer.py to not be able to read sw results from this cache file.')
parent_parser.add_argument('--ig-sw-binary', default=partis_dir + '/packages/ig-sw/src/ig_align/ig-sw', help='Path to ig-sw executable.')
parent_parser.add_argument('--vsearch-binary',  help='Path to vsearch binary (vsearch binaries for linux and darwin are pre-installed in bin/, but for other systems you need to get your own)')
parent_parser.add_argument('--is-simu', action='store_true', help='Set if running on simulated sequences')
parent_parser.add_argument('--skip-unproductive', action='store_true', help='Skip sequences which Smith-Waterman determines to be unproductive (i.e. if they have stop codons, out of frame cdr3, or mutated cyst/tryp/phen)')
parent_parser.add_argument('--also-remove-duplicate-sequences-with-different-lengths', action='store_true', help='By default we remove any queries which have exactly the same sequence as a previous query. If this is set, we also consider as duplicates sequences which are sub/super strings of previous sequences (we keep the longest one).')
parent_parser.add_argument('--dont-remove-framework-insertions', action='store_true', help='By default we trim anything to the 5\' of V and 3\' of J in order to remove queries with identical coding regions. This turns that off.')
parent_parser.add_argument('--dont-rescale-emissions', action='store_true', help='Don\'t scale each hmm\'s emission probabilities to account for the branch length of each individual sequence.')
parent_parser.add_argument('--no-indels', action='store_true', help='Tell smith-waterman not to look for indels, by drastically increasing the gap-open penalty (you can also set the penalty directly).')
parent_parser.add_argument('--seed', type=int, default=int(time.time()), help='Random seed for use (mostly) by recombinator (to allow reproducibility)')
parent_parser.add_argument('--min-observations-to-write', type=int, default=20, help='When writing hmm model files, if we see a gene version fewer times than this, we average over other alleles, or other primary versions, etc. (see hmmwriter). NOTE default is manipulated in partitiondriver.py')
parent_parser.add_argument('--no-per-base-mfreqs', action='store_true')
parent_parser.add_argument('--region-end-exclusion-length', type=int, default=0, help='when counting/writing parameters, ignore this many bases abutting non-templated insertions for calculating mutation frequencies (note: doesn\'t make a difference (hence set to 0 by default) probably because we\'re setting a strongish prior on these bases when writing hmms anyway')

parent_parser.add_argument('--only-genes', help='Colon-separated list of genes to which to restrict the analysis. If any regions (V/D/J) are not represented among these genes, these regions are left unrestricted.')
parent_parser.add_argument('--n-max-per-region', default='3:5:2', help='Number of best smith-waterman matches (per region, in the format v:d:j) to pass on to the hmm')
parent_parser.add_argument('--gap-open-penalty', type=int, default=30, help='Penalty for indel creation in Smith-Waterman step.')
parent_parser.add_argument('--initial-match-mismatch', default='5:1', help='Initial match:mismatch scores for smith-waterman (we run several iterations, increasing mismatch each time).')
parent_parser.add_argument('--max-vj-mut-freq', type=float, default=0.4, help='skip sequences whose mutation rates in V and J are greater than this (it\'s really not possible to get meaningful smith-waterman matches above this)')
parent_parser.add_argument('--max-logprob-drop', type=float, default=5., help='stop glomerating when the total logprob has dropped by this much')
parent_parser.add_argument('--n-simultaneous-seqs', type=int, help='Number of simultaneous sequences on which to run the multi-HMM (e.g. 2 for a pair hmm)')
parent_parser.add_argument('--simultaneous-true-clonal-seqs', action='store_true', help='Run true clonal sequences together simultaneously with the multi-HMM.')

parent_parser.add_argument('--infname', help='input sequence file in .fa, .fq or .csv (if .csv, specify id string and sequence headers with --name-column and --seq-column)')
parent_parser.add_argument('--name-column', help='csv column name for sequence ids')
parent_parser.add_argument('--seq-column', help='csv column name for nucleotide sequences')
parent_parser.add_argument('--outfname', help='output file name')
parent_parser.add_argument('--presto-output', action='store_true', help='write output file in presto format')
parent_parser.add_argument('--linearham', action='store_true', help='write hmm input file in linearham format')
parent_parser.add_argument('--cluster-annotation-fname', help='output file for cluster annotations (default is <--outfname>-cluster-annotations.csv)')
parent_parser.add_argument('--parameter-dir', help='Directory to/from which to write/read sample-specific parameters. If not specified, a default location is used (and printed to std out). If it does not exist, we infer parameters before proceeding to the desired action.')
parent_parser.add_argument('--parameter-type', default='hmm', choices=('sw', 'hmm'), help='Use parameters from Smith-Waterman (sw) or the HMM (hmm) subdirectories for inference/simulation? (you should almost certainly use the hmm ones, but sw is occasionally useful for debugging)')
parent_parser.add_argument('--persistent-cachefname', help='Name of file which will be used as an initial cache file (if it exists), and to which all cached info will be written out before exiting.')
parent_parser.add_argument('--sw-cachefname', help='Smith-Waterman cache file name. Default is set using a hash of all the input sequence ids (in partitiondriver, since we have to read the input file first).')
parent_parser.add_argument('--workdir', help='Temporary working directory (default is set below)')

parent_parser.add_argument('--plotdir', help='Base directory to which to write plots (by default this is not set, and consequently no plots are written')
parent_parser.add_argument('--plot-performance', action='store_true', help='renamed to --plot-annotation-performance')
parent_parser.add_argument('--plot-annotation-performance', action='store_true', help='Write out plots comparing true and inferred annotation accuracy distributions')
parent_parser.add_argument('--only-csv-plots', action='store_true', help='only write csv plots (svg writing is pretty slow, and it\'s frequently better to make comparison plots from csvs with ./bin/compare-plotdirs.py anyway)')
parent_parser.add_argument('--only-overall-plots', action='store_true', help='don\'t write per-gene plots (it\'s kind of slow writing that many plots)')

parent_parser.add_argument('--default-initial-germline-dir', default=partis_dir + '/data/germlines', help='For internal use only. To specify your own germline directory from which to start, use --initial-germline-dir instead.')
parent_parser.add_argument('--initial-germline-dir', help='Directory with fastas from which to read germline set. Only used when caching parameters, during which its contents is copied into --parameter-dir, perhaps (i.e. if specified) with modification. NOTE default is set below, because control flow is too complicated for argparse')
parent_parser.add_argument('--simulation-germline-dir', help='Germline directory that was used for simulation (only used when --plot-annotation-performance is set)')
parent_parser.add_argument('--aligned-germline-fname', help='fasta file with alignments for each V gene')

parent_parser.add_argument('--n-alleles-per-gene', type=int, default=None, help='number of alleles to assume per gene when removing spurious alleles during parameter caching (default is set below -- 1 if looking for new alleles, otherwise 2) NOTE difference to --n-sim-alleles-per-gene')
parent_parser.add_argument('--min-allele-prevalence-fraction', type=float, default=0.0005, help='Remove any alleles that represent less than this fraction of the repertoire.')
parent_parser.add_argument('--n-max-total-alleles', type=int, help='maximum number of alleles per segment to allow when removing alleles (presumably just used for testing).')

parent_parser.add_argument('--leave-default-germline', action='store_true')
parent_parser.add_argument('--dont-remove-unlikely-alleles', action='store_true')
parent_parser.add_argument('--allele-cluster', action='store_true')
parent_parser.add_argument('--dont-find-new-alleles', action='store_true')
# parent_parser.add_argument('--always-find-new-alleles', action='store_true', help='By default we only look for new alleles if a repertoire\'s mutation rate is amenable to reasonable new-allele sensitivity (i.e. if it\'s not crazy high). This overrides that.')
parent_parser.add_argument('--debug-allele-finding', action='store_true', help='print lots of debug info on new-allele fits')
parent_parser.add_argument('--new-allele-fname', help='fasta fname to which to write any new alleles (they are also written, together with previously-known alleles that are also in the sample, to --parameter-dir)')
parent_parser.add_argument('--n-max-allele-finding-iterations', type=int, default=1, help='maximum number of times to look for new alleles')
parent_parser.add_argument('--n-max-new-alleles-per-gene-per-iteration', type=int, default=3, help='with an option name that long...')
parent_parser.add_argument('--n-max-snps', type=int, default=8, help='when new-allele finding, look for new alleles separated from existing alleles by up to this many SNPs')
parent_parser.add_argument('--n-max-mutations-per-segment', type=int, default=23, help='when new-allele finding, exclude sequences which have more than this many mutations in the V segment')
parent_parser.add_argument('--min-allele-finding-gene-length', type=int, default=150, help='if (after excluding particularly short reads) the reads for a gene are shorter than this, then don\'t look for new alleles with/on this gene')
parent_parser.add_argument('--plot-and-fit-absolutely-everything', type=int, help='fit every single position for this <istart> and write every single corresponding plot (slow as hell, and only for debugging/making plots for paper)')

parent_parser.add_argument('--seed-unique-id', help='Only look for sequences that are clonally related to this unique id. Much much much faster than partitioning the entire dataset.')
parent_parser.add_argument('--seed-seq', help='same as --seed-unique-id, but specifies a sequence that doesn\'t have to be in the original file')

# ----------------------------------------------------------------------------------------
subconfig = {
    'version'          : {'func' : int, 'help' : 'print version information and exit'},
    'cache-parameters' : {'func' : run_partitiondriver, 'help' : 'Cache parameter values and write hmm model files.'},
    'annotate'         : {'func' : run_partitiondriver, 'help' : 'Annotate sequences in input file, i.e. run the viterbi algorithm, using pre-existing parameter directory.'},
    'partition'        : {'func' : run_partitiondriver, 'help' : 'Partition sequences in input file into clonally-related families using pre-existing parameter directory.'},
    'simulate'         : {'func' : run_simulation,      'help' : 'Generate simulated sequences based on information in pre-existing parameter directory.'},
    'view-annotations' : {'func' : view_existing_output,'help' : 'Print (to std out) the annotations from an existing annotation output csv.'},
    'view-partitions'  : {'func' : view_existing_output,'help' : 'Print (to std out) the partitions from an existing partition output csv.'},
    'plot-partitions'  : {'func' : view_existing_output,'help' : 'Plot existing partitions and cluster annotations.'},
    'view-alternative-naive-seqs'  : {'func' : view_existing_output,'help' : 'Print (to std out) a comparison of the naive sequences corresponding to sub- and super-clusters of the cluster specified with --queries. You must have specified --calculate-alternative-naive-seqs (or --persistent-cachefname) in a previous partition step so that this information was saved.'},
    'run-viterbi'      : {'func' : run_partitiondriver, 'help' : 'deprecated, do not use'},
}

subargs = {subname : [] for subname in subconfig}

subargs['partition'].append({'name' : '--naive-hamming', 'kwargs' : {'action' : 'store_true', 'help' : 'agglomerate purely with naive hamming distance, i.e. set the low and high preclustering bounds to the same value'}})
subargs['partition'].append({'name' : '--naive-vsearch', 'kwargs' : {'action' : 'store_true', 'help' : 'Very fast clustering: infer naive (unmutated ancestor) for each input sequence, then toss it all into vsearch. But, of course, not as accurate as the slower methods.'}})
# subargs['partition'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'Only look for sequences that are clonally related to this unique id. Much much much faster than partitioning the entire dataset.'}})
# subargs['partition'].append({'name' : '--seed-seq', 'kwargs' : {'help' : 'same as --seed-unique-id, but specifies a sequence that doesn\'t have to be in the original file'}})
subargs['partition'].append({'name' : '--annotation-clustering', 'kwargs' : {'help' : 'Perform annotation-based clustering: group together sequences with the same V and J, same CDR3 length, and 90%% cdr identity. Very, very inaccurate.'}})
subargs['partition'].append({'name' : '--annotation-clustering-thresholds', 'kwargs' : {'default' : '0.9', 'help' : 'colon-separated list of thresholds for annotation-based (e.g. vollmers) clustering'}})
subargs['partition'].append({'name' : '--print-cluster-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'deprecated'}})
subargs['partition'].append({'name' : '--naive-hamming-bounds', 'kwargs' : {'help' : 'Clustering bounds (lo:hi colon-separated pair) on naive sequence hamming distance. If not specified, the bounds are set based on the per-dataset mutation levels. For most purposes should be left at the defaults.'}})
subargs['partition'].append({'name' : '--logprob-ratio-threshold', 'kwargs' : {'type' : float, 'default' : 18., 'help' : 'reaches a min value of <this> minus five for large clusters.'}})
subargs['partition'].append({'name' : '--synthetic-distance-based-partition', 'kwargs' : {'action' : 'store_true', 'help' : 'Use simulation truth info to create a synthetic distance-based partition (for validation).'}})
subargs['partition'].append({'name' : '--cache-naive-hfracs', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to naive sequences and log probabilities, also cache naive hamming fractions between cluster pairs. Only really useful for plotting or testing.'}})
subargs['partition'].append({'name' : '--n-precache-procs', 'kwargs' : {'type' : int, 'help' : 'Number of processes to use when precaching naive sequences. Default is set based on some heuristics, and should typically only be overridden for testing.'}})
subargs['partition'].append({'name' : '--biggest-naive-seq-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--biggest-logprob-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--n-partitions-to-write', 'kwargs' : {'type' : int, 'default' : 10, 'help' : 'Number of partitions (surrounding the best partition) to write to output file.'}})
subargs['partition'].append({'name' : '--naive-swarm', 'kwargs' : {'action' : 'store_true', 'help' : 'Use swarm instead of vsearch, which the developer recommends. Didn\'t seem to help much, and needs more work to optimize threshold, so DO NOT USE.'}})
subargs['partition'].append({'name' : '--random-seed-seq', 'kwargs' : {'action' : 'store_true', 'help' : 'choose a sequence at random from the input sequences, and use it as a seed for seed partitioning'}})
subargs['partition'].append({'name' : '--small-clusters-to-ignore', 'kwargs' : {'help' : 'colon-separated list (or dash-separated inclusive range) of cluster sizes to throw out after several partition steps. E.g. \'1:2\' will, after <--n-steps-at-which-to-ignore-small-clusters> partition steps, throw out all singletons and pairs. Alternatively, \'1-10\' will ignore all clusters with size less than 11.'}})
subargs['partition'].append({'name' : '--n-steps-after-which-to-ignore-small-clusters', 'kwargs' : {'type' : int, 'default' : 3, 'help' : 'number of partition steps after which to throw out small clusters (where "small" is controlled by <--small-clusters-to-ignore>). (They\'re thrown out before this if we get to n_procs one before this).'}})
subargs['partition'].append({'name' : '--n-final-clusters', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and there are still more than this many clusters, attempt to keep merging until there aren\'t. NOTE should\'ve called this --n-max-final-clusters, but too late to change now'}})
subargs['partition'].append({'name' : '--min-largest-cluster-size', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and the largest cluster isn\'t this big, attempt to keep merging until it is.'}})
subargs['partition'].append({'name' : '--n-partition-subsets', 'kwargs' : {'type' : int, 'help' : 'compare the partitions of this many independent subsets, each of size <--n-max-queries>'}})
subargs['partition'].append({'name' : '--calculate-alternative-naive-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'write to disk all the information necessary to, in a later step, print alternative inferred naive sequences (i.e. visualize uncertainty in the inferred naive sequence). All this really does is set --persistent-cachefname, i.e. copy the hmm cache file that we would anyway be making (but deleting) to somewhere sensible for later use.'}})
subargs['partition'].append({'name' : '--max-cluster-size', 'kwargs' : {'type' : int, 'help' : 'stop clustering immediately if any cluster grows larger than this (useful for limiting memory usage, which can become a problem when the final partition contains very large clusters)'}})
subargs['partition'].append({'name' : '--write-additional-cluster-annotations', 'kwargs' : {'help' : 'in addition to writing annotations for each cluster in the best partition, also write annotations for several partitions on either side of the best partition. Specified as a pair of numbers \'m:n\' for m partitions before, and n partitions after, the best partition.'}})

subargs['simulate'].append({'name' : '--mutation-multiplier', 'kwargs' : {'type' : float, 'help' : 'Multiply observed branch lengths by some factor when simulating, e.g. if in data it was 0.05, but you want closer to ten percent in your simulation, set this to 2'}})
subargs['simulate'].append({'name' : '--mimic-data-read-length', 'kwargs' : {'action' : 'store_true', 'help' : 'trim V 5\' and D 3\' to mimic read lengths seen in data'}})
subargs['simulate'].append({'name' : '--n-sim-events', 'kwargs' : {'type' : int, 'default' : 1, 'help' : 'Number of rearrangement events to simulate'}})
subargs['simulate'].append({'name' : '--n-trees', 'kwargs' : {'type' : int, 'help' : 'Number of phylogenetic trees from which to choose during simulation (we pre-generate this many trees before starting a simulation run, then for each rearrangement event choose one at random -- so this should be at least of order the number of simulated events, so your clonal families don\'t all have the same tree).'}})
subargs['simulate'].append({'name' : '--n-leaves', 'kwargs' : {'type' : float, 'default' : 5., 'help' : 'Parameter describing the number of leaves per tree (maybe the mean, maybe not -- depends on the distribution)'}})
subargs['simulate'].append({'name' : '--constant-number-of-leaves', 'kwargs' : {'action' : 'store_true', 'help' : 'Give all trees the same number of leaves'}})
subargs['simulate'].append({'name' : '--n-leaf-distribution', 'kwargs' : {'default' : 'geometric', 'choices' : ['geometric', 'box', 'zipf'], 'help' : 'distribution from which to draw the number of leaves for each tree'}})
subargs['simulate'].append({'name' : '--indel-frequency', 'kwargs' : {'default' : 0., 'type' : float, 'help' : 'fraction of simulated sequences with indels'}})
subargs['simulate'].append({'name' : '--mean-indels-per-indeld-seq', 'kwargs' : {'default' : 1.2, 'type' : float, 'help' : 'mean number of indels in each sequence which we\'ve already decided has indels (geometric distribution)'}})
subargs['simulate'].append({'name' : '--mean-indel-length', 'kwargs' : {'default' : 5, 'help' : 'mean length of each indel (geometric distribution)'}})
subargs['simulate'].append({'name' : '--indel-location', 'kwargs' : {'choices' : [None, 'v', 'cdr3'], 'help' : 'where to put the indels'}})
# NOTE command to generate gtr parameter file: [stoat] partis/ > zcat /shared/silo_researcher/Matsen_F/MatsenGrp/data/bcr/output_sw/A/04-A-M_gtr_tr-qi-gi.json.gz | jq .independentParameters | grep -v '[{}]' | sed 's/["\:,]//g' | sed 's/^[ ][ ]*//' | sed 's/ /,/' | sort >data/gtr.txt)
subargs['simulate'].append({'name' : '--gtrfname', 'kwargs' : {'default' : partis_dir + '/data/recombinator/gtr.txt', 'help' : 'File with list of GTR parameters. Fed into bppseqgen along with the chosen tree. Corresponds to an arbitrary dataset at the moment, but eventually will be inferred per-dataset.'}})
subargs['simulate'].append({'name' : '--rearrange-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for rearrangement-level parameters, and instead make up some plausible stuff from scratch (unless --mutate-from-scratch is also specified, mutation info is taken from --scratch-mute-freq-dir, i.e. mutations are *not* from scratch)'}})
subargs['simulate'].append({'name' : '--mutate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for shm-level (mutation) parameters, and instead make up some plausible stuff from scratch'}})
subargs['simulate'].append({'name' : '--flat-mute-freq', 'kwargs' : {'type' : float, 'default' : 0.05, 'help' : 'constant mutation frequency, across all positions for all regions and inserts, which is enforced if --mutate-from-scratch is set'}})
subargs['simulate'].append({'name' : '--scratch-mute-freq-dir', 'kwargs' : {'default' : partis_dir + '/data/recombinator/scratch-parameters', 'help' : 'synthetic/partial parameter directory with only shm-level (mutation) information, which allows to specify --rearrange-from-scratch without also setting --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Choose a subset of the available genes to represent this sample\'s germline.'}})
subargs['simulate'].append({'name' : '--n-genes-per-region', 'kwargs' : {'default' : glutils.default_n_genes_per_region, 'help' : 'colon-separated list specifying the number of genes (not alleles -- i.e. the *total* number of alleles is this times the number of alleles per gene) for each region (for use with --generate-germline-set)'}})
subargs['simulate'].append({'name' : '--n-sim-alleles-per-gene', 'kwargs' : {'default' : glutils.default_n_alleles_per_gene, 'help' : 'colon-separated list of mean alleles per gene for each region (for use with --generate-germline-set).'}})
subargs['simulate'].append({'name' : '--min-sim-allele-prevalence-freq', 'kwargs' : {'default' : glutils.default_min_allele_prevalence_freq,'type' : float, 'help' : 'minimum frequency at which alleles are allowed to occur, e.g. if it\'s 0.01 then each pair of V alleles will have a prevalence ratio between 0.01 and 1'}})
subargs['simulate'].append({'name' : '--allele-prevalence-fname', 'kwargs' : {'help' : 'abandon help all ye who enter here'}})
subargs['simulate'].append({'name' : '--root-mrca-weibull-parameter', 'kwargs' : {'type' : float, 'help' : 'if set, uses TreeSimGM (instead of TreeSim), and value passed as the parameter (e.g. 0.1: long root-mrca distance, lots of shared mutation; 5: short, little) NOTE requires installation of TreeSimGM'}})

subargs['simulate'].append({'name' : '--subsimproc', 'kwargs' : {'action' : 'store_true', 'help' : 'set to true if this process has subsidiary simulation processes handled by Popen'}})
subargs['simulate'].append({'name' : '--im-a-subproc', 'kwargs' : {'action' : 'store_true', 'help' : 'set to true if this is a sub-process handled by --subsimproc'}})

subparsermap = {}
for name, vals in subconfig.items():
    subparsermap[name] = subparsers.add_parser(name, parents=[parent_parser], help=vals['help'], formatter_class=MultiplyInheritedFormatter)
    subparsermap[name].set_defaults(func=vals['func'])
    for argconf in subargs[name]:
        subparsermap[name].add_argument(argconf['name'], **argconf['kwargs'])

# ----------------------------------------------------------------------------------------
if '--action' in sys.argv:
    raise Exception('--action is now a positional argument')

args = parser.parse_args()

if args.action == 'run-viterbi':
    print'  note: replacing deprecated action name \'run-viterbi\' with current name \'annotate\' (this doesn\'t change any actual behavior)'
    args.action = 'annotate'

args.partis_dir = partis_dir

# add OR of all arguments to all subparsers to <args>, as None (to avoid having to rewrite a *##!(%ton of other code)
for name in subconfig:
    for argconf in subargs[name]:
        if argconf['name'][:2] != '--':
            raise Exception('expected argument %s to be of form --<stuff>' % argconf['name'])
        argname = argconf['name'][2:].replace('-', '_')
        if argname not in args.__dict__:
            args.__dict__[argname] = None

if args.chain is not None:
    print '    note: transferring argument from deprecated option \'--chain %s\' to new option \'--locus %s\'' % (args.chain, 'ig' + args.chain)
    args.locus = 'ig' + args.chain
    args.chain = None
args.only_genes = utils.get_arg_list(args.only_genes)
args.n_procs = utils.get_arg_list(args.n_procs, intify=True)
args.n_fewer_procs = args.n_procs[0] if len(args.n_procs) == 1 else args.n_procs[1]
args.n_procs = args.n_procs[0]
args.queries = utils.get_arg_list(args.queries)
args.queries_to_include = utils.get_arg_list(args.queries_to_include)
args.reco_ids = utils.get_arg_list(args.reco_ids)
args.istartstop = utils.get_arg_list(args.istartstop, intify=True)
if args.istartstop is not None:
    if args.istartstop[0] >= args.istartstop[1] or args.istartstop[0] < 0:
        raise Exception('invalid --istartstop specification: %d %d' % (args.istartstop[0], args.istartstop[1]))
args.n_max_per_region = utils.get_arg_list(args.n_max_per_region, intify=True)
args.write_additional_cluster_annotations = utils.get_arg_list(args.write_additional_cluster_annotations, intify=True)
if args.write_additional_cluster_annotations is not None and len(args.write_additional_cluster_annotations) != 2:
    raise Exception('--write-additional-cluster-annotations must be specified as two numbers \'m:n\', but I got %s' % args.write_additional_cluster_annotations)

args.region_end_exclusions = {r : [args.region_end_exclusion_length if ('%s_%s' % (r, e)) in utils.real_erosions else 0 for e in ['5p', '3p']] for r in utils.regions}
args.region_end_exclusion_length = None  # there isn't really a big reason to set it to None, but this makes clear that I should only be using the dict version

args.initial_match_mismatch = utils.get_arg_list(args.initial_match_mismatch, intify=True)
args.annotation_clustering_thresholds = utils.get_arg_list(args.annotation_clustering_thresholds, floatify=True)
args.naive_hamming_bounds = utils.get_arg_list(args.naive_hamming_bounds, floatify=True)
if args.small_clusters_to_ignore is not None:
    if '-' in args.small_clusters_to_ignore:
        lo, hi = [int(cluster_size) for cluster_size in args.small_clusters_to_ignore.split('-')]
        args.small_clusters_to_ignore = range(lo, hi + 1)
    else:
        args.small_clusters_to_ignore = utils.get_arg_list(args.small_clusters_to_ignore, intify=True)
if args.seed_unique_id is not None:
    args.seed_unique_id = args.seed_unique_id.strip()  # protect against the space you may put in front of it if it's got an initial minus sign (better way is to use an equals sign)
    if args.queries is not None and args.seed_unique_id not in args.queries:
        raise Exception('seed uid %s not in --queries %s' % (args.seed_unique_id, ' '.join(args.queries)))
    if args.random_seed_seq:
        raise Exception('can\'t specify both --seed-unique-id and --random-seed-seq')
if args.sw_debug is None:  # if not explicitly set, set equal to regular debug
    args.sw_debug = args.debug

if args.only_genes is not None:
    for gene in args.only_genes:  # make sure they're all at least valid ig genes
        utils.split_gene(gene)

# if n_procs < 1 or n_procs > 9999:  # It happened, at least once. You know, probably.
#     raise Exception('bad n_procs %s' % n_procs)
if args.n_procs > args.n_max_procs:
    print 'reducing n procs %d to --n-max-procs %d' % (args.n_procs, args.n_max_procs)
    args.n_procs = args.n_max_procs
if args.n_fewer_procs > args.n_max_procs:
    print 'reducing n procs %d to --n-max-procs %d' % (args.n_fewer_procs, args.n_max_procs)
    args.n_fewer_procs = args.n_max_procs

if args.print_git_commit or args.action == 'version':
    print 'RUN ' + ' '.join(sys.argv)
    tag = check_output(['git', 'tag']).split()[-1]
    print '       tag %s' % tag
    print '    commit %s' % check_output(['git', 'rev-parse', 'HEAD']).strip()
    if args.action == 'version':
        sys.exit(0)

args.is_data = not args.is_simu  # whole code base uses is_data, this is better than changing all of that

if args.simultaneous_true_clonal_seqs:
    if args.is_data:
        raise Exception('can only pass true clonal families to multi-hmm together on simulation and with --is-simu set')
    if args.n_simultaneous_seqs is not None:
        raise Exception('can\'t specify both --n-simultaneous-seqs and --simultaneous-true-clonal-seqs')

if args.no_indels and args.gap_open_penalty < 1000:
    print 'forcing --gap-open-penalty to 1000 to prevent indels, since --no-indels was specified (you can also adjust this penalty directly)'
    args.gap_open_penalty = 1000

if 'tr' in args.locus and args.mutation_multiplier is None:
    args.mutation_multiplier = 0.

if args.workdir is None:  # set default here so we know whether it was set by hand or not
    def choose_random_subdir(dirname):
        subname = str(random.randint(0, 999999))
        while os.path.exists(dirname + '/' + subname):
            subname = str(random.randint(0, 999999))
        return dirname + '/' + subname
    if args.batch_system is not None and os.path.exists('/fh/fast/matsen_e'):
        args.workdir = choose_random_subdir('/fh/fast/matsen_e/' + os.path.basename(os.getenv('HOME')) + '/_tmp/hmms')
    else:
        args.workdir = choose_random_subdir('/tmp/' + os.path.basename(os.getenv('HOME')) + '/hmms')
        if args.batch_system is not None:
            print '  %s: using batch system %s with default --workdir (%s) -- if this isn\'t visible to the batch nodes on your system, you\'ll need to change it' % (utils.color('red', 'warning'), args.batch_system, args.workdir)
else:
    args.workdir = args.workdir.rstrip('/')
if os.path.exists(args.workdir):
    raise Exception('workdir %s already exists' % args.workdir)

if args.batch_system == 'sge' and args.batch_options is not None:
    if '-e' in args.batch_options or '-o' in args.batch_options:
        print '%s --batch-options contains \'-e\' or \'-o\', but we add these automatically since we need to be able to parse each job\'s stdout and stderr. You can control the directory under which they\'re written with --workdir (which is currently %s).' % (utils.color('red', 'warning'), args.workdir)

if args.cluster_annotation_fname is None and args.outfname is not None:
    args.cluster_annotation_fname = args.outfname.replace(utils.getsuffix(args.outfname), '-cluster-annotations.csv')

if args.calculate_alternative_naive_seqs or (args.action == 'view-alternative-naive-seqs' and args.persistent_cachefname is None):
    if args.outfname is None:
        raise Exception('have to specify --outfname in order to calculate alternative naive sequences')
    args.persistent_cachefname = args.outfname.replace('.csv', '-hmm-cache.csv')
    if args.calculate_alternative_naive_seqs and os.path.exists(args.persistent_cachefname):
        if os.stat(args.persistent_cachefname).st_size == 0:
            print '  note: removing existing zero-length persistent cache file %s' % args.persistent_cachefname
            os.remove(args.persistent_cachefname)
        else:
            raise Exception('persistent cache file %s already exists, but we were asked to --calculate-alternative-naive-seqs. Either it\'s an old file (in which case you should delete it), or you\'ve already got the alternative annotations (so you can just run view-alternative-naive-seqs)' % args.persistent_cachefname)

if args.plot_performance:
    print '%s encountered deprecated argument --plot-performance, moving value to --plot-annotation-performance' % utils.color('yellow', 'warning')
    args.plot_annotation_performance = True
if args.plot_annotation_performance:
    if args.plotdir is None:
        raise Exception('can\'t plot performance unless --plotdir is specified')
    if not args.is_simu:
        raise Exception('can\'t plot performance unless --is-simu is set')

if args.parameter_type != 'hmm':
    print '  using non-default parameter type \'%s\'' % args.parameter_type

if args.presto_output and args.aligned_germline_fname is None:
    raise Exception('in order to get presto output, you have to set --aligned-germline-fname (a fasta file with germline alignments for every germline gene)')

if args.parameter_dir is not None:
    args.parameter_dir = args.parameter_dir.rstrip('/')

if args.count_parameters and not args.dont_write_parameters:
    raise Exception('if you set --count-parameters, you should also set --dont-write-parameters to make sure you\'re not accidentally overwriting existing parameters ')

if os.path.exists(args.default_initial_germline_dir + '/' + args.species):  # ick that is hackey
    args.default_initial_germline_dir += '/' + args.species

if args.n_max_snps is not None and args.n_max_mutations_per_segment is not None:
    if args.n_max_snps > args.n_max_mutations_per_segment - 10:
        raise Exception('--n-max-snps should be at least ten less than --n-max-mutations-per-segment, but I got %d and %d' % (args.n_max_snps, args.n_max_mutations_per_segment))

if args.n_alleles_per_gene is None:
    if not args.dont_find_new_alleles:
        args.n_alleles_per_gene = 1
    else:
        args.n_alleles_per_gene = 2

if args.leave_default_germline:
    args.dont_remove_unlikely_alleles = True
    args.allele_cluster = False
    args.dont_find_new_alleles = True

# ----------------------------------------------------------------------------------------
random.seed(args.seed)
numpy.random.seed(args.seed)
start = time.time()
args.func(args)
print '      total time: %.1f' % (time.time()-start)
